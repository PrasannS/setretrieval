{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "026d85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylate import indexes, models, retrieve\n",
    "from setretrieval.utils.utils import pickload\n",
    "from statistics import mean\n",
    "from matplotlib import pyplot as plt\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from tqdm import tqdm\n",
    "from setretrieval.utils.utils import pickdump\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e3ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load_from_disk(\"../data/datastores/wikipedia_docs_15k\")\n",
    "# try to take another node...\n",
    "question_set = DatasetDict.load_from_disk(\"../data/colbert_training/gemini_ntrain_ptest\")['train'].select(range(5200, 10000))\n",
    "res = pickload(\"../cache/gendata/passagesearchtrain_v1_5.2k_0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb538940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_to_inds(clist):\n",
    "    # given list of booleans, return indices where True\n",
    "    return [i for i, c in enumerate(clist) if c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e4a1240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to indices: 100%|██████████| 2347/2347 [01:07<00:00, 35.01it/s]\n"
     ]
    }
   ],
   "source": [
    "allpos = []\n",
    "for r in tqdm(res, desc=\"Converting to indices\"):\n",
    "    # each r is a list of \n",
    "    inds = chunks_to_inds(r)\n",
    "    allpos.append([dataset[ind]['text'] for ind in inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f848304",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickdump(allpos, \"../cache/gendata/passagesearchtrain_v1_5.2k_4Bpos.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "928caf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "alltext = [dataset[ind]['text'] for ind in range(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd5837c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2347\n"
     ]
    }
   ],
   "source": [
    "print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c37026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sum(r) for r in res].count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb383dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ColBERT(\n",
    "    model_name_or_path=\"../cache/colbert_training/contrastive-Qwen_Qwen¬3-Embedding-0.6B-bs8-e11-lr3e-05-gemini_ntrain_ptest/checkpoint-500\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de0a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = indexes.PLAID(\n",
    "    index_folder=\"../cache/colbert_indices\",\n",
    "    index_name=\"indextest\",\n",
    "    override=False\n",
    ")\n",
    "\n",
    "retriever = retrieve.ColBERT(index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_ids = [\"1\", \"2\", \"3\"]\n",
    "\n",
    "documents = [\n",
    "    \"ColBERT’s late-interaction keeps token-level embeddings to deliver cross-encoder-quality ranking at near-bi-encoder speed, enabling fine-grained relevance, robustness across domains, and hardware-friendly scalable search.\",\n",
    "\n",
    "    \"PLAID compresses ColBERT token vectors via product quantization to shrink storage by 10×, uses two-stage centroid scoring for sub-200 ms latency, and plugs directly into existing ColBERT pipelines.\",\n",
    "\n",
    "    \"PyLate is a library built on top of Sentence Transformers, designed to simplify and optimize fine-tuning, inference, and retrieval with state-of-the-art ColBERT models. It enables easy fine-tuning on both single and multiple GPUs, providing flexibility for various hardware setups. PyLate also streamlines document retrieval and allows you to load a wide range of models, enabling you to construct ColBERT models from most pre-trained language models.\",\n",
    "]\n",
    "\n",
    "# Encode the documents\n",
    "documents_embeddings = model.encode(\n",
    "    documents,\n",
    "    batch_size=1,\n",
    "    is_query=False, # Encoding documents\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "# Add the documents ids and embeddings to the PLAID index\n",
    "index.add_documents(\n",
    "    documents_ids=documents_ids,\n",
    "    documents_embeddings=documents_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_embeddings = model.encode(\n",
    "    [\"query for document 3\", \"query for document 1\"],\n",
    "    batch_size=32,\n",
    "    is_query=True, # Encoding queries\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "scores = retriever.retrieve(\n",
    "    queries_embeddings=queries_embeddings,\n",
    "    k=10\n",
    ")\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaling2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
