{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194b86bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/projects/sewonm/prasann/.conda/envs/scaling2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# toydata to test simple algorithm\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "from setretrieval.utils.utils import pickload, pickdump\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6606889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in some seed documents (from wikipedia)\n",
    "wikidocs = Dataset.load_from_disk(\"../propercache/data/datastores/wikipedia_docs_10k_decont\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ee5c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 6941.61it/s]\n",
      "Map (num_proc=1): 100%|██████████| 62598/62598 [00:09<00:00, 6508.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from setretrieval.datagen.toydata import chunk_ds, process_ngrams\n",
    "\n",
    "n = 1\n",
    "chunked_wikidocs = chunk_ds(wikidocs, 10)\n",
    "ngrammed_wikidocs = chunked_wikidocs.map(lambda x: process_ngrams(x, n), num_proc=1)\n",
    "allngs_flat = [n for ng in ngrammed_wikidocs['pos_chunks'] for n in ng]\n",
    "counts = Counter(allngs_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d3fd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eval positives...\n",
      "Creating negative pool...\n",
      "Creating count bins for balanced sampling...\n",
      "Creating train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49500/49500 [00:59<00:00, 836.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating eval set...\n",
      "\n",
      "Train set statistics:\n",
      "Average positive count: 50.69 (median: 24.00)\n",
      "Average negative count: 53.19 (median: 32.00)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def create_train_eval_splits_ultra_fast(ngrammed_dataset, counts, train_ratio=0.8, experquery=1, negatives_per_positive=1, seed=42, train_samples=50000, max_negatives_pool=100000):\n",
    "    \"\"\"\n",
    "    Ultra-fast version with balanced count sampling for negatives.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n_samples = train_samples\n",
    "    all_indices = np.arange(n_samples)\n",
    "    np.random.shuffle(all_indices)\n",
    "    \n",
    "    split_point = int(n_samples * train_ratio)\n",
    "    train_indices = all_indices[:split_point]\n",
    "    eval_indices = all_indices[split_point:]\n",
    "    \n",
    "    # Collect eval positives\n",
    "    print(\"Collecting eval positives...\")\n",
    "    eval_positives = set()\n",
    "    for idx in eval_indices:\n",
    "        eval_positives.update(ngrammed_dataset[idx]['pos_chunks'])\n",
    "    \n",
    "    # Create a large pool of potential negatives (sampled once)\n",
    "    print(\"Creating negative pool...\")\n",
    "    all_ngrams = []\n",
    "    for row in ngrammed_dataset['pos_chunks']:\n",
    "        all_ngrams.extend(row)\n",
    "    \n",
    "    # Sample a subset for faster operations\n",
    "    if len(all_ngrams) > max_negatives_pool:\n",
    "        negative_pool = random.sample(all_ngrams, max_negatives_pool)\n",
    "    else:\n",
    "        negative_pool = all_ngrams\n",
    "    \n",
    "    # Create count-based bins for stratified sampling\n",
    "    print(\"Creating count bins for balanced sampling...\")\n",
    "    unique_negatives = list(set(negative_pool))\n",
    "    negative_counts_map = {ng: counts.get(ng, 1) for ng in unique_negatives}\n",
    "    \n",
    "    # Group negatives by count ranges (log-scale bins)\n",
    "    from collections import defaultdict\n",
    "    count_bins = defaultdict(list)\n",
    "    for ng in unique_negatives:\n",
    "        count = negative_counts_map[ng]\n",
    "        bin_key = int(np.log10(max(count, 1)))  # log10 binning\n",
    "        count_bins[bin_key].append(ng)\n",
    "    \n",
    "    bin_keys = list(count_bins.keys())\n",
    "    \n",
    "    print(\"Creating train set...\")\n",
    "    train_data = []\n",
    "    \n",
    "    for idx in tqdm(train_indices):\n",
    "        row = ngrammed_dataset[int(idx)]\n",
    "        query = row['text']\n",
    "        pos_chunks = row['pos_chunks']\n",
    "        \n",
    "        # Filter valid positives\n",
    "        pos_set = set(pos_chunks)\n",
    "        valid_positives = [p for p in pos_chunks if p not in eval_positives]\n",
    "        \n",
    "        if not valid_positives:\n",
    "            continue\n",
    "        \n",
    "        # Compute target count range for negatives based on positives\n",
    "        pos_counts = [counts.get(p, 1) for p in valid_positives]\n",
    "        target_bin = int(np.log10(max(np.mean(pos_counts), 1)))\n",
    "        \n",
    "        # Sample negatives from similar count bins\n",
    "        n_needed = len(valid_positives) * negatives_per_positive\n",
    "        negatives = []\n",
    "        \n",
    "        # Try target bin first, then expand to nearby bins\n",
    "        search_bins = [target_bin]\n",
    "        for offset in range(1, len(bin_keys) + 1):\n",
    "            if target_bin + offset in bin_keys:\n",
    "                search_bins.append(target_bin + offset)\n",
    "            if target_bin - offset in bin_keys:\n",
    "                search_bins.append(target_bin - offset)\n",
    "        \n",
    "        for bin_key in search_bins:\n",
    "            if len(negatives) >= n_needed:\n",
    "                break\n",
    "            \n",
    "            candidates = count_bins[bin_key]\n",
    "            random.shuffle(candidates)\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                if candidate not in pos_set:\n",
    "                    negatives.append(candidate)\n",
    "                    if len(negatives) >= n_needed:\n",
    "                        break\n",
    "        \n",
    "        # Fallback: if not enough negatives, sample from any bin\n",
    "        if len(negatives) < n_needed:\n",
    "            attempts = 0\n",
    "            max_attempts = n_needed * 10\n",
    "            while len(negatives) < n_needed and attempts < max_attempts:\n",
    "                candidate = random.choice(unique_negatives)\n",
    "                if candidate not in pos_set and candidate not in negatives:\n",
    "                    negatives.append(candidate)\n",
    "                attempts += 1\n",
    "        \n",
    "        # Create examples\n",
    "        for i, pos in enumerate(random.sample(valid_positives, k=min(experquery, len(valid_positives)))):\n",
    "            for j in range(min(negatives_per_positive, len(negatives) - i * negatives_per_positive)):\n",
    "                if i * negatives_per_positive + j < len(negatives):\n",
    "                    train_data.append({\n",
    "                        'query': query,\n",
    "                        'positive': pos,\n",
    "                        'negative': negatives[i * negatives_per_positive + j]\n",
    "                    })\n",
    "    \n",
    "    # Create eval set\n",
    "    print(\"Creating eval set...\")\n",
    "    eval_data = [\n",
    "        {\n",
    "            'question': ngrammed_dataset[int(idx)]['text'],\n",
    "            'pos_chunks': ngrammed_dataset[int(idx)]['pos_chunks']\n",
    "        }\n",
    "        for idx in eval_indices\n",
    "    ]\n",
    "    \n",
    "    # Print statistics\n",
    "    if train_data:\n",
    "        pos_counts = [counts.get(d['positive'], 1) for d in train_data]\n",
    "        neg_counts = [counts.get(d['negative'], 1) for d in train_data]\n",
    "        print(f\"\\nTrain set statistics:\")\n",
    "        print(f\"Average positive count: {np.mean(pos_counts):.2f} (median: {np.median(pos_counts):.2f})\")\n",
    "        print(f\"Average negative count: {np.mean(neg_counts):.2f} (median: {np.median(neg_counts):.2f})\")\n",
    "\n",
    "    # print number of positives from the test set that are in the train test\n",
    "    test_positives = set(eval_positives)\n",
    "    train_positives = set([r['positive'] for r in train_data])\n",
    "    print(len(test_positives & train_positives))\n",
    "    \n",
    "    return Dataset.from_list(train_data), Dataset.from_list(eval_data)\n",
    "\n",
    "# Usage\n",
    "tsamps = 50000\n",
    "train_eval = create_train_eval_splits_ultra_fast(\n",
    "    ngrammed_wikidocs, \n",
    "    counts,\n",
    "    train_ratio=0.99, \n",
    "    train_samples=tsamps, \n",
    "    negatives_per_positive=1, \n",
    "    seed=42, \n",
    "    max_negatives_pool=40000\n",
    ")\n",
    "train_ds, eval_ds = train_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27cdf6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating negative pool...\n",
      "Creating count bins for balanced sampling...\n",
      "Creating train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49500/49500 [00:06<00:00, 7970.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating eval set...\n",
      "\n",
      "Train set statistics:\n",
      "Average positive count: 20504.40 (median: 956.00)\n",
      "Average negative count: 19973.60 (median: 13571.00)\n",
      "2815\n"
     ]
    }
   ],
   "source": [
    "def create_train_eval_splits_ultra_fast_contaminated(ngrammed_dataset, counts, train_ratio=0.8, experquery=1, negatives_per_positive=1, seed=42, train_samples=50000, max_negatives_pool=100000):\n",
    "    \"\"\"\n",
    "    Ultra-fast version with balanced count sampling for negatives.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n_samples = train_samples\n",
    "    all_indices = np.arange(n_samples)\n",
    "    np.random.shuffle(all_indices)\n",
    "    \n",
    "    split_point = int(n_samples * train_ratio)\n",
    "    train_indices = all_indices[:split_point]\n",
    "    eval_indices = all_indices[split_point:]\n",
    "    \n",
    "    # Create a large pool of potential negatives (sampled once)\n",
    "    print(\"Creating negative pool...\")\n",
    "    all_ngrams = []\n",
    "    for row in ngrammed_dataset['pos_chunks']:\n",
    "        all_ngrams.extend(row)\n",
    "    \n",
    "    # Sample a subset for faster operations\n",
    "    if len(all_ngrams) > max_negatives_pool:\n",
    "        negative_pool = random.sample(all_ngrams, max_negatives_pool)\n",
    "    else:\n",
    "        negative_pool = all_ngrams\n",
    "    \n",
    "    # Create count-based bins for stratified sampling\n",
    "    print(\"Creating count bins for balanced sampling...\")\n",
    "    unique_negatives = list(set(negative_pool))\n",
    "    negative_counts_map = {ng: counts.get(ng, 1) for ng in unique_negatives}\n",
    "    \n",
    "    # Group negatives by count ranges (log-scale bins)\n",
    "    from collections import defaultdict\n",
    "    count_bins = defaultdict(list)\n",
    "    for ng in unique_negatives:\n",
    "        count = negative_counts_map[ng]\n",
    "        bin_key = int(np.log10(max(count, 1)))  # log10 binning\n",
    "        count_bins[bin_key].append(ng)\n",
    "    \n",
    "    bin_keys = list(count_bins.keys())\n",
    "    \n",
    "    print(\"Creating train set...\")\n",
    "    train_data = []\n",
    "    \n",
    "    for idx in tqdm(train_indices):\n",
    "        row = ngrammed_dataset[int(idx)]\n",
    "        query = row['text']\n",
    "        pos_chunks = row['pos_chunks']\n",
    "        \n",
    "        # Use all positives (no filtering)\n",
    "        pos_set = set(pos_chunks)\n",
    "        valid_positives = pos_chunks\n",
    "        \n",
    "        if not valid_positives:\n",
    "            continue\n",
    "        \n",
    "        # Compute target count range for negatives based on positives\n",
    "        pos_counts = [counts.get(p, 1) for p in valid_positives]\n",
    "        target_bin = int(np.log10(max(np.mean(pos_counts), 1)))\n",
    "        \n",
    "        # Sample negatives from similar count bins\n",
    "        n_needed = len(valid_positives) * negatives_per_positive\n",
    "        negatives = []\n",
    "        \n",
    "        # Try target bin first, then expand to nearby bins\n",
    "        search_bins = [target_bin]\n",
    "        for offset in range(1, len(bin_keys) + 1):\n",
    "            if target_bin + offset in bin_keys:\n",
    "                search_bins.append(target_bin + offset)\n",
    "            if target_bin - offset in bin_keys:\n",
    "                search_bins.append(target_bin - offset)\n",
    "        \n",
    "        for bin_key in search_bins:\n",
    "            if len(negatives) >= n_needed:\n",
    "                break\n",
    "            \n",
    "            candidates = count_bins[bin_key]\n",
    "            random.shuffle(candidates)\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                if candidate not in pos_set:\n",
    "                    negatives.append(candidate)\n",
    "                    if len(negatives) >= n_needed:\n",
    "                        break\n",
    "        \n",
    "        # Fallback: if not enough negatives, sample from any bin\n",
    "        if len(negatives) < n_needed:\n",
    "            attempts = 0\n",
    "            max_attempts = n_needed * 10\n",
    "            while len(negatives) < n_needed and attempts < max_attempts:\n",
    "                candidate = random.choice(unique_negatives)\n",
    "                if candidate not in pos_set and candidate not in negatives:\n",
    "                    negatives.append(candidate)\n",
    "                attempts += 1\n",
    "        \n",
    "        # Create examples\n",
    "        for i, pos in enumerate(random.sample(valid_positives, k=min(experquery, len(valid_positives)))):\n",
    "            for j in range(min(negatives_per_positive, len(negatives) - i * negatives_per_positive)):\n",
    "                if i * negatives_per_positive + j < len(negatives):\n",
    "                    train_data.append({\n",
    "                        'query': query,\n",
    "                        'positive': pos,\n",
    "                        'negative': negatives[i * negatives_per_positive + j]\n",
    "                    })\n",
    "    \n",
    "    # Create eval set\n",
    "    print(\"Creating eval set...\")\n",
    "    eval_data = [\n",
    "        {\n",
    "            'question': ngrammed_dataset[int(idx)]['text'],\n",
    "            'pos_chunks': ngrammed_dataset[int(idx)]['pos_chunks']\n",
    "        }\n",
    "        for idx in eval_indices\n",
    "    ]\n",
    "    \n",
    "    # Print statistics\n",
    "    if train_data:\n",
    "        pos_counts = [counts.get(d['positive'], 1) for d in train_data]\n",
    "        neg_counts = [counts.get(d['negative'], 1) for d in train_data]\n",
    "        print(f\"\\nTrain set statistics:\")\n",
    "        print(f\"Average positive count: {np.mean(pos_counts):.2f} (median: {np.median(pos_counts):.2f})\")\n",
    "        print(f\"Average negative count: {np.mean(neg_counts):.2f} (median: {np.median(neg_counts):.2f})\")\n",
    "\n",
    "    # print how much overlap there is between train and eval positives\n",
    "    train_positives = set([r['positive'] for r in train_data])\n",
    "    eval_positives = set([item for sublist in [r['pos_chunks'] for r in eval_data] for item in sublist])\n",
    "    print(len(train_positives & eval_positives))\n",
    "    \n",
    "    return Dataset.from_list(train_data), Dataset.from_list(eval_data)\n",
    "\n",
    "# Usage\n",
    "tsamps = 50000\n",
    "train_eval = create_train_eval_splits_ultra_fast_contaminated(\n",
    "    ngrammed_wikidocs, \n",
    "    counts,\n",
    "    train_ratio=0.99, \n",
    "    train_samples=tsamps, \n",
    "    negatives_per_positive=1, \n",
    "    seed=42, \n",
    "    max_negatives_pool=40000\n",
    ")\n",
    "train_ds, eval_ds = train_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89108fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contam = True\n",
    "if contam:\n",
    "    suff = \"balancedcontam\"\n",
    "else:\n",
    "    suff = \"balanced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd262557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 48500/48500 [00:00<00:00, 454674.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 180322.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 500/500 [00:00<00:00, 87217.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = DatasetDict({\n",
    "    'train': train_ds.select(range(1000, len(train_ds))),\n",
    "    'test': train_ds.select(range(1000))\n",
    "})\n",
    "train_ds.save_to_disk(f\"../propercache/data/colbert_training/wiki{n}gramtrain{tsamps}sample{suff}\")\n",
    "eval_ds.save_to_disk(f\"../propercache/data/evalsets/evalwiki{n}grameval{tsamps}samples{suff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed1d765a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 7390.85it/s]\n",
      "Map (num_proc=1): 100%|██████████| 62598/62598 [00:09<00:00, 6279.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4827 11026 45173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 50000/50000 [00:00<00:00, 2785099.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def dsets_to_dstore(n, samps, angs=None, dssize=50000):\n",
    "    traindata = DatasetDict.load_from_disk(f\"../propercache/data/colbert_training/wiki{n}gramtrain{samps}sample{suff}\")['train']\n",
    "    evaldata = Dataset.load_from_disk(f\"../propercache/data/evalsets/evalwiki{n}grameval{samps}samples{suff}\")\n",
    "\n",
    "    # get all pos_chuhnks from evaldata\n",
    "    allpos = [r['pos_chunks'] for r in evaldata]\n",
    "    allpos = list(set([item for sublist in allpos for item in sublist]))\n",
    "    # get all positives from traindata\n",
    "    allpos_train = list(set([r['positive'] for r in traindata]))\n",
    "    remaining = dssize - len(allpos)\n",
    "    print(len(allpos), len(allpos_train), remaining)\n",
    "    assert remaining > 0\n",
    "    if angs is not None:\n",
    "        aset = set(angs)\n",
    "        aset = list(aset - set(allpos))\n",
    "    else:\n",
    "        aset = list(set(allpos_train) - set(allpos))\n",
    "    usepos = random.sample(aset, k=remaining)\n",
    "    tdata = Dataset.from_list([{'text': p} for p in usepos + allpos])\n",
    "    return tdata\n",
    "\n",
    "n = 1\n",
    "chunked_wikidocs = chunk_ds(wikidocs, 10)\n",
    "ngrammed_wikidocs = chunked_wikidocs.map(lambda x: process_ngrams(x, n), num_proc=1)\n",
    "allngs_flat = [n for ng in ngrammed_wikidocs['pos_chunks'] for n in ng]\n",
    "counts = Counter(allngs_flat)\n",
    "dscount=50000\n",
    "tdstore = dsets_to_dstore(n, dscount, allngs_flat)\n",
    "tdstore.save_to_disk(f\"../propercache/data/datastores/wiki{n}gramdstore{dscount}{suff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eb46910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526235"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1484cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in stuff for inverted task\n",
    "t100krand = DatasetDict.load_from_disk(\"../propercache/data/colbert_training/nountrain100000minimal10dwords/\")\n",
    "testdstpre = Dataset.load_from_disk(\"../propercache/data/datastores/evaltdstore10words50pos100k\")\n",
    "testset = Dataset.load_from_disk(\"../propercache/data/evalsets/testset10words50pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb691b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctxt = set(testdstpre['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([\" \" in row['text'] for row in nouns100k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee3d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # do just once to avoid extra I/O\n",
    "    all_nouns = [word for synset in wn.all_synsets('n') for word in synset.lemma_names()]\n",
    "    all_nouns = [word for word in all_nouns if \"_\" not in word]\n",
    "    all_nouns = list(set(all_nouns))\n",
    "    random.shuffle(all_nouns)\n",
    "    test_nouns = [{'text': word} for word in all_nouns[:1000]]\n",
    "    # allnouns = Dataset.from_list([{\"text\": word} for word in all_nouns])\n",
    "    # allnouns.save_to_disk(\"../propercache/data/datastores/allnouns\")\n",
    "    nouns100k = Dataset.from_list([{\"text\": word} for word in all_nouns[1000:101000]])\n",
    "    nouns10k = Dataset.from_list([{\"text\": word} for word in all_nouns[1000:11000]])\n",
    "    Dataset.from_list(test_nouns).save_to_disk(\"../propercache/data/datastores/heldoutnouns\")\n",
    "    nouns100k.save_to_disk(\"../propercache/data/datastores/nouns100k\")\n",
    "    nouns10k.save_to_disk(\"../propercache/data/datastores/nouns10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98bffe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nouns = Dataset.load_from_disk(\"../propercache/data/datastores/heldoutnouns\")\n",
    "nouns100k = Dataset.load_from_disk(\"../propercache/data/datastores/nouns100k\")\n",
    "nouns10k = Dataset.load_from_disk(\"../propercache/data/datastores/nouns10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WE WILL HAVE 2 KINDS OF TASKS\n",
    "# A: Given a query (one word), return a list of chunks (each chunk has several words)\n",
    "# B: Given a query (a list of words), return a list of chunks (each chunk is just one noun at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0861ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOW STARTING SETUP FOR A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7821831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48349/100000 [00:02<00:02, 20094.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m nouns10k_texts \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m nouns10k]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_batch_rand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnouns10k_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatapoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating test data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m test_data \u001b[38;5;241m=\u001b[39m generate_batch_rand(nouns10k_texts, \u001b[38;5;241m1000\u001b[39m, dwords)\n",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m, in \u001b[0;36mgenerate_batch_rand\u001b[0;34m(wset_texts, count, doc_words)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(count)):\n\u001b[1;32m     40\u001b[0m     query \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(wset_texts)\n\u001b[0;32m---> 41\u001b[0m     chunkspos \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwset_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m chunkspos:\n\u001b[1;32m     44\u001b[0m         chunkspos[random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, doc_words\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m query\n",
      "File \u001b[0;32m~/.conda/envs/scaling2/lib/python3.10/random.py:519\u001b[0m, in \u001b[0;36mRandom.choices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    517\u001b[0m     floor \u001b[38;5;241m=\u001b[39m _floor\n\u001b[1;32m    518\u001b[0m     n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m    \u001b[38;5;66;03m# convert to float for a small speed improvement\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [population[floor(random() \u001b[38;5;241m*\u001b[39m n)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _repeat(\u001b[38;5;28;01mNone\u001b[39;00m, k)]\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     cum_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(_accumulate(weights))\n",
      "File \u001b[0;32m~/.conda/envs/scaling2/lib/python3.10/random.py:519\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    517\u001b[0m     floor \u001b[38;5;241m=\u001b[39m _floor\n\u001b[1;32m    518\u001b[0m     n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m    \u001b[38;5;66;03m# convert to float for a small speed improvement\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [population[floor(random() \u001b[38;5;241m*\u001b[39m n)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _repeat(\u001b[38;5;28;01mNone\u001b[39;00m, k)]\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     cum_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(_accumulate(weights))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generate_train_query_rand(wset, doc_words=10):\n",
    "    query = random.choice(wset)['text']\n",
    "    chunkspos = [r['text'] for r in random.choices(wset, k=doc_words)]\n",
    "    \n",
    "    # Faster check and swap\n",
    "    if query not in chunkspos:\n",
    "        chunkspos[random.randint(0, doc_words-1)] = query\n",
    "    \n",
    "    chunksneg = [r['text'] for r in random.choices(wset, k=doc_words)]\n",
    "    \n",
    "    # Join once at the end\n",
    "    return {'query': query, 'positive': \" \".join(chunkspos), 'negative': \" \".join(chunksneg)}\n",
    "\n",
    "def generate_train_query_minimal(wset, doc_words=10):\n",
    "    query = random.choice(wset)['text']\n",
    "    \n",
    "    # Use random.sample directly instead of converting to set\n",
    "    chunks = [r['text'] for r in random.sample(wset, k=doc_words)]\n",
    "    \n",
    "    # Check and replace if needed\n",
    "    try:\n",
    "        idx = chunks.index(query)\n",
    "        chunks[idx] = random.choice(wset)['text']\n",
    "    except ValueError:\n",
    "        pass  # query not in chunks, which is fine\n",
    "    \n",
    "    negdata = \" \".join(chunks)\n",
    "    \n",
    "    # Swap for positive\n",
    "    chunks[random.randint(0, doc_words-1)] = query\n",
    "    posdata = \" \".join(chunks)\n",
    "    \n",
    "    return {'query': query, 'positive': posdata, 'negative': negdata}\n",
    "\n",
    "\n",
    "# Batch generation function - much faster\n",
    "def generate_batch_rand(wset_texts, count, doc_words=10):\n",
    "    results = []\n",
    "    for _ in tqdm(range(count)):\n",
    "        query = random.choice(wset_texts)\n",
    "        chunkspos = random.choices(wset_texts, k=doc_words)\n",
    "        \n",
    "        if query not in chunkspos:\n",
    "            chunkspos[random.randint(0, doc_words-1)] = query\n",
    "        \n",
    "        chunksneg = random.choices(wset_texts, k=doc_words)\n",
    "        \n",
    "        results.append({\n",
    "            'query': query, \n",
    "            'positive': \" \".join(chunkspos), \n",
    "            'negative': \" \".join(chunksneg)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Generate in larger batches\n",
    "dwords = 250\n",
    "datapoints = 100000\n",
    "\n",
    "\n",
    "# Pre-extract text once to avoid repeated dictionary access\n",
    "nouns10k_texts = [item['text'] for item in nouns10k]\n",
    "print(\"Generating training data...\")\n",
    "train_data = generate_batch_rand(nouns10k_texts, datapoints, dwords)\n",
    "print(\"Generating test data...\")\n",
    "test_data = generate_batch_rand(nouns10k_texts, 1000, dwords)\n",
    "\n",
    "train100krand = DatasetDict({\n",
    "    'train': Dataset.from_list(train_data),\n",
    "    'test': Dataset.from_list(test_data)\n",
    "})\n",
    "do_save=True\n",
    "if do_save:\n",
    "    train100krand.save_to_disk(f\"../propercache/data/colbert_training/v2nountrain{datapoints}rand{dwords}dwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99da0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train100krand['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_query_rand(wset, doc_words=10):\n",
    "    query = random.choice(wset)['text']\n",
    "    chunkspos = [r['text'] for r in random.choices(wset, k=doc_words)]\n",
    "    # if query not in chunkspos, randomly swap one of the chunks with query\n",
    "    if query not in chunkspos:\n",
    "        swap_idx = random.randint(0, len(chunkspos)-1)\n",
    "        chunkspos[swap_idx] = query\n",
    "    chunksneg = [r['text'] for r in random.choices(wset, k=doc_words)]\n",
    "    return {'query': query, 'positive': \" \".join(chunkspos), 'negative': \" \".join(chunksneg)}\n",
    "\n",
    "# TODO this will help us test if minimality somehow makes data better for \"hard negatives\"\n",
    "# - Note this is pretty clean minimality, other stuff could / could not matter\n",
    "def generate_train_query_minimal(wset, doc_words=10):\n",
    "    query = random.choice(wset)['text']\n",
    "    chunks = set([r['text'] for r in random.sample(wset, k=doc_words)])\n",
    "    # TODO sanity check that some weird alphabetization isn't doing something weird\n",
    "    # make sure query is not in chunks\n",
    "    if query in chunks:\n",
    "        chunks.remove(query)\n",
    "        chunks.add(random.choice(wset)['text'])\n",
    "    negdata = \" \".join(chunks)\n",
    "    # now randomly swap one of the chunks with query\n",
    "    swap_idx = random.randint(0, len(chunks)-1)\n",
    "    chunks = list(chunks)\n",
    "    chunks[swap_idx] = query\n",
    "    posdata = \" \".join(chunks)\n",
    "    return {'query': query, 'positive': posdata, 'negative': negdata}\n",
    "\n",
    "# get 100k train sets for both kinds\n",
    "dwords = 250\n",
    "datapoints = 100000\n",
    "train100krand = DatasetDict({\n",
    "    'train': Dataset.from_list([generate_train_query_rand(nouns10k, dwords) for _ in tqdm(range(datapoints))]),\n",
    "    'test': Dataset.from_list([generate_train_query_rand(nouns10k, dwords) for _ in tqdm(range(1000))])\n",
    "})\n",
    "# train100kminimal = DatasetDict({\n",
    "#     'train': Dataset.from_list([generate_train_query_minimal(nouns10k, dwords) for _ in tqdm(range(datapoints))]),\n",
    "#     'test': Dataset.from_list([generate_train_query_minimal(nouns10k, dwords) for _ in tqdm(range(1000))])\n",
    "# })\n",
    "do_save = True\n",
    "if do_save:\n",
    "    train100krand.save_to_disk(f\"../propercache/data/colbert_training/v2nountrain{datapoints}rand{dwords}dwords\")\n",
    "    # train100kminimal.save_to_disk(f\"../propercache/data/colbert_training/nountrain{datapoints}minimal{dwords}dwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4a8388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 55505.04it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 18301.19it/s]\n"
     ]
    }
   ],
   "source": [
    "def singwordquery_eval(evalnouns, trainnouns, dstoresize=10000, tsetsize=500, num_pos=50, docwords=10):\n",
    "    evsetnouns = set([r['text'] for r in evalnouns])\n",
    "    trsetnouns = set([r['text'] for r in trainnouns])\n",
    "    assert len(evsetnouns & trsetnouns) == 0\n",
    "    assert all([\" \" not in r['text'] for r in evalnouns])\n",
    "    assert all([\" \" not in r['text'] for r in trainnouns])\n",
    "    evalnouns = list([r['text'] for r in evalnouns])\n",
    "    trainnouns = list([r['text'] for r in trainnouns])\n",
    "    # make eval test set and datastore\n",
    "    testqueries = random.sample(evalnouns, k=tsetsize)\n",
    "    # make eval datastore, with each document have docwords words\n",
    "    starterdocs = [[r for r in random.sample(trainnouns, k=docwords)] for _ in tqdm(range(dstoresize))]\n",
    "    # for each query, randomly choose num_pos docs from starterdocs. For each of these randomly replace one word with query word\n",
    "    query_poschunks = []\n",
    "    indlist = list(range(dstoresize))\n",
    "    for query in tqdm(testqueries):\n",
    "        posinds = random.sample(indlist, k=num_pos)\n",
    "        for posind in posinds:\n",
    "            # randomly replace one word in starterdocs[posind] with query word\n",
    "            wordind = random.randint(0, docwords-1)\n",
    "            starterdocs[posind][wordind] = query\n",
    "        query_poschunks.append(posinds)\n",
    "    doc_poschunks = Dataset.from_list([{\"text\": \" \".join(d)} for d in starterdocs])\n",
    "    query_data = Dataset.from_list([\n",
    "        {\n",
    "            \"question\": q, \n",
    "            \"pos_chunks\": [doc_poschunks[p]['text'] for p in query_poschunks[i]], \n",
    "            \"num_pos_chunks\": len(query_poschunks[i])\n",
    "        } \n",
    "        for i, q in enumerate(testqueries)])\n",
    "    return doc_poschunks, query_data\n",
    "\n",
    "dwords = 50\n",
    "npos=50\n",
    "# get a test set to work with \n",
    "tdstore, testset10words50pos = singwordquery_eval(test_nouns, nouns10k, 100000, 500, npos, dwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e20e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 100000/100000 [00:00<00:00, 196931.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 500/500 [00:00<00:00, 4723.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tdstore.save_to_disk(f\"../propercache/data/datastores/v2evaltdstore{dwords}words{npos}pos100k\")\n",
    "testset10words50pos.save_to_disk(f\"../propercache/data/evalsets/v2testset{dwords}words{npos}pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tns = set([r['text'] for r in test_nouns])\n",
    "trainnns = set(nouns100k['text'])\n",
    "print(len(tns), len(trainnns), len(tns & trainnns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee8889",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdstore = Dataset.load_from_disk(\"../propercache/data/datastores/v2evaltdstore50words50pos100k\")\n",
    "testset10words50pos = Dataset.load_from_disk(\"../propercache/data/evalsets/v2testset50words50pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdstore = Dataset.load_from_disk(\"../propercache/data/datastores/v2evaltdstore50words50pos100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa580d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset10words50pos[8]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([row['text'] for row in tdstore if f\" tram \" in row['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc579e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcnts = []\n",
    "qs = list(testset10words50pos['question'])\n",
    "for q in tqdm(qs):\n",
    "    wordcnts.append(sum([f\" {q} \" in row['text'] for row in tdstore]))\n",
    "    print(wordcnts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4810946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdstore.save_to_disk(\"../propercache/data/datastores/evaltdstore10words50pos100k\")\n",
    "# testset10words50pos.save_to_disk(\"../propercache/data/evalsets/testset10words50pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make eval datastore (main thing is making datastore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapoints = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc69419",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOW STARTING SETUP FOR B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eebf1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_toy_query(wset, wset_texts, query_words=100, ndps=1, datatype=\"train\"):\n",
    "    querywords = random.choices(wset, k=query_words)\n",
    "    querywords_text = [q['text'] for q in querywords]\n",
    "    query = \" \".join(querywords_text)\n",
    "    poslist = random.choices(querywords_text, k=ndps)\n",
    "    \n",
    "    # Convert to set for O(1) lookup\n",
    "    querywords_set = set(querywords_text)\n",
    "    \n",
    "    # Sample negatives - just keep trying with random.choice until we get valid ones\n",
    "    neglist = []\n",
    "    for _ in range(ndps):\n",
    "        while True:\n",
    "            neg = random.choice(wset_texts)\n",
    "            if neg not in querywords_set:\n",
    "                neglist.append(neg)\n",
    "                break\n",
    "    \n",
    "    if datatype == \"train\":\n",
    "        return [{'query': query, 'positive': pos, 'negative': neg} \n",
    "                for pos, neg in zip(poslist, neglist)]\n",
    "    else:\n",
    "        return {'question': query, 'pos_chunks': querywords_text, \n",
    "                'numposchunks': len(querywords_text)}\n",
    "\n",
    "def toyquerydset(wset, qwords, ndps, tsize):\n",
    "    # Pre-extract all texts from wset ONCE\n",
    "    wset_texts = [w['text'] for w in wset]\n",
    "    \n",
    "    alldata = []\n",
    "    inddps = tsize // ndps\n",
    "    for _ in tqdm(range(inddps)):\n",
    "        alldata.extend(construct_toy_query(wset, wset_texts, qwords, ndps, \"train\"))\n",
    "    return Dataset.from_list(alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a23e02ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:38<00:00, 1017.98it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1015.25it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100000/100000 [00:00<00:00, 193094.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 114937.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "numwords = 50\n",
    "tset = 100000\n",
    "dp_per_query = 1\n",
    "traindata = DatasetDict({\n",
    "    'train': toyquerydset(nouns10k, numwords, dp_per_query, tset),\n",
    "    'test': toyquerydset(nouns10k, numwords, dp_per_query, 1000)\n",
    "})\n",
    "traindata.save_to_disk(f\"../propercache/data/colbert_training/v2nountraining{numwords}words{tset}ndps{dp_per_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc1dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET EVAL DATA\n",
    "evallists = [50] # [1, 5, 10, 25]\n",
    "tnountexts = [r['text'] for r in test_nouns]\n",
    "for ev in evallists:\n",
    "    evdata = Dataset.from_list([construct_toy_query(test_nouns, tnountexts, query_words=ev, datatype=\"eval\") for _ in tqdm(range(500))])\n",
    "    if False:\n",
    "        evdata.save_to_disk(f\"../propercache/data/evalsets/v2nountest{ev}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48702939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate eval data following a probability distribution over count of query words\n",
    "def pdquerydset(wset, qwdistdict, tsize, ndps=1):\n",
    "    alldata = []\n",
    "    dcnt = 0\n",
    "    while dcnt < tsize:\n",
    "        qw = random.choices(list(qwdistdict.keys()), weights=list(qwdistdict.values()), k=1)[0]\n",
    "        alldata.extend(construct_toy_query(wset, qw, min(ndps, qw), \"train\"))\n",
    "        dcnt += 1\n",
    "        if dcnt % 10000 == 0:\n",
    "            print(f\"Generated {dcnt} queries\")\n",
    "    return Dataset.from_list(alldata)\n",
    "\n",
    "# uniform distribution from 5 to 100\n",
    "qwdistdictuni = {i: 1/96 for i in range(5, 101)}\n",
    "# power law distribution from 5 to 100\n",
    "qwdistdictpower = {i: i**(-0.5) for i in range(5, 101)}\n",
    "\n",
    "# plot power law distribution in qwdistdictpower\n",
    "x = list(qwdistdictpower.keys())\n",
    "y = list(qwdistdictpower.values())\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Number of Words in Query\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Power Law Distribution of Query Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 100k train set for each distribution\n",
    "train100kuniform = pdquerydset(nouns10k, qwdistdictuni, 100000)\n",
    "test100kuniform = pdquerydset(nouns10k, qwdistdictuni, 1000)\n",
    "\n",
    "train100kpower = pdquerydset(nouns10k, qwdistdictpower, 100000)\n",
    "test100kpower = pdquerydset(nouns10k, qwdistdictpower, 1000)\n",
    "\n",
    "DatasetDict({\n",
    "    'train': train100kuniform,\n",
    "    'test': test100kuniform\n",
    "}).save_to_disk(\"../propercache/data/colbert_training/nountraining100kuniform5_100\")\n",
    "\n",
    "DatasetDict({\n",
    "    'train': train100kpower,\n",
    "    'test': test100kpower\n",
    "}).save_to_disk(\"../propercache/data/colbert_training/nountraining100kpower5_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9051b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check distributions of 'query' length in train100kuniform, train100kpower\n",
    "plt.hist([len(q['query'].split()) for q in train100kuniform], alpha=0.5, label='Uniform', range=(5, 100))\n",
    "plt.hist([len(q['query'].split()) for q in train100kpower], alpha=0.5, label='Power', range=(5, 100))\n",
    "plt.ylabel(\"Count\") \n",
    "plt.xlabel(\"Number of Words in Query\")\n",
    "plt.title(\"Distribution of Query Length in Training Sets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "evallists = [1, 5, 10, 25]\n",
    "testsets = {}\n",
    "for ev in evallists:\n",
    "    testsets[ev] = Dataset.load_from_disk(f\"../propercache/data/evalsets/nountest{ev}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "allnouns = Dataset.load_from_disk(\"../propercache/data/datastores/allnouns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa27ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ansfixed = list(set([a['text'] for a in allnouns]))\n",
    "ansfixed = Dataset.from_list([{'text': a} for a in ansfixed])\n",
    "ansfixed.save_to_disk(\"../propercache/data/datastores/allnounsfixed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb69efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = list([a['text'] for a in allnouns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29235147",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ans), len(set(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a test set, convert it to a train set\n",
    "def test_to_trainset(tset):\n",
    "    alldata = []\n",
    "    for row in tset:\n",
    "        for pos in row['pos_chunks']:\n",
    "            alldata.append({'query': row['question'], 'positive': pos, 'negative': random.choice(allnouns)['text']})\n",
    "    return DatasetDict({'train': Dataset.from_list(alldata), 'test': Dataset.from_list(alldata)})\n",
    "\n",
    "tset = 25\n",
    "test_to_trainset(testsets[tset]).save_to_disk(f\"../propercache/data/colbert_training/nountestset{tset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398076b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_trainset(testsets[tset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32066cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval10 = Dataset.from_list([construct_toy_query(test_nouns, 10, \"eval\") for _ in tqdm(range(1000))])\n",
    "eval100 = Dataset.from_list([construct_toy_query(test_nouns, 100, \"eval\") for _ in tqdm(range(1000))])\n",
    "\n",
    "eval10.save_to_disk(\"../propercache/data/evalsets/nountest10/\")\n",
    "eval100.save_to_disk(\"../propercache/data/evalsets/nountest100/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4352c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO need to make eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata['train'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c2e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaling2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
