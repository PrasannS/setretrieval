{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toydata to test simple algorithm\n",
    "from datasets import Dataset, DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b90a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dstore = Dataset.load_from_disk(\"../propercache/data/datastores/v2evaltdstore50words50pos100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54255f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dstore2 = Dataset.load_from_disk(\"../propercache/data/datastores/evaltdstore10words50pos100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28daf90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evset = Dataset.load_from_disk(\"../propercache/data/evalsets/v2testset150words50pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78332638",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadeb = \"../propercache/data/colbert_training/train50wordsdebug100\"\n",
    "datadeb = DatasetDict.load_from_disk(datadeb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca2f0a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1query = DatasetDict.load_from_disk(\"../propercache/data/colbert_training/v2nountrain100000rand10dwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b52f328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting data into set format to work with\n",
    "def proc_query_setversion(row):\n",
    "    negwords = set(row['negative'].split())\n",
    "    qp = \"||||\".join([rp for rp in row['positive'].split()])\n",
    "    print(qp.count(\"||||\"))\n",
    "    row['query'] = qp\n",
    "    return row\n",
    "    \n",
    "def proc_word_setversion(row):\n",
    "    row['positive'] = row['query'].replace(\" \", \"||||\")\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b7569a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 4222.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 4365.61 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1078.34 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 4563.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 4156.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "task1samp = DatasetDict.load_from_disk(\"../propercache/data/colbert_training/v2nountrain100000rand10dwords\")\n",
    "task1samp = DatasetDict({\n",
    "    \"train\": task1samp[\"train\"].select(range(100)),\n",
    "    \"test\": task1samp[\"test\"].select(range(100))\n",
    "})\n",
    "task1samp.save_to_disk(\"../propercache/data/colbert_training/10dworddebug100\")\n",
    "task1samp[\"train\"] = task1samp[\"train\"].map(proc_query_setversion)\n",
    "task1samp.save_to_disk(\"../propercache/data/colbert_training/10dworddebug100setversion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc95ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 6331.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "datadeb['train'] = datadeb['train'].map(proc_word_setversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4661650f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 12714.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 13854.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "datadeb.save_to_disk(\"../propercache/data/colbert_training/train50wordsdebug100setversion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1951b128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'coelenteron heterosexuality rassling hydra lingering PM prohibition schtik issuance show'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dstore2[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ee5c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 6941.61it/s]\n",
      "Map (num_proc=1): 100%|██████████| 62598/62598 [00:09<00:00, 6508.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from setretrieval.datagen.toydata import chunk_ds, process_ngrams\n",
    "\n",
    "n = 1\n",
    "chunked_wikidocs = chunk_ds(wikidocs, 10)\n",
    "ngrammed_wikidocs = chunked_wikidocs.map(lambda x: process_ngrams(x, n), num_proc=1)\n",
    "allngs_flat = [n for ng in ngrammed_wikidocs['pos_chunks'] for n in ng]\n",
    "counts = Counter(allngs_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d3fd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eval positives...\n",
      "Creating negative pool...\n",
      "Creating count bins for balanced sampling...\n",
      "Creating train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49500/49500 [00:59<00:00, 836.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating eval set...\n",
      "\n",
      "Train set statistics:\n",
      "Average positive count: 50.69 (median: 24.00)\n",
      "Average negative count: 53.19 (median: 32.00)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def create_train_eval_splits_ultra_fast(ngrammed_dataset, counts, train_ratio=0.8, experquery=1, negatives_per_positive=1, seed=42, train_samples=50000, max_negatives_pool=100000):\n",
    "    \"\"\"\n",
    "    Ultra-fast version with balanced count sampling for negatives.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n_samples = train_samples\n",
    "    all_indices = np.arange(n_samples)\n",
    "    np.random.shuffle(all_indices)\n",
    "    \n",
    "    split_point = int(n_samples * train_ratio)\n",
    "    train_indices = all_indices[:split_point]\n",
    "    eval_indices = all_indices[split_point:]\n",
    "    \n",
    "    # Collect eval positives\n",
    "    print(\"Collecting eval positives...\")\n",
    "    eval_positives = set()\n",
    "    for idx in eval_indices:\n",
    "        eval_positives.update(ngrammed_dataset[idx]['pos_chunks'])\n",
    "    \n",
    "    # Create a large pool of potential negatives (sampled once)\n",
    "    print(\"Creating negative pool...\")\n",
    "    all_ngrams = []\n",
    "    for row in ngrammed_dataset['pos_chunks']:\n",
    "        all_ngrams.extend(row)\n",
    "    \n",
    "    # Sample a subset for faster operations\n",
    "    if len(all_ngrams) > max_negatives_pool:\n",
    "        negative_pool = random.sample(all_ngrams, max_negatives_pool)\n",
    "    else:\n",
    "        negative_pool = all_ngrams\n",
    "    \n",
    "    # Create count-based bins for stratified sampling\n",
    "    print(\"Creating count bins for balanced sampling...\")\n",
    "    unique_negatives = list(set(negative_pool))\n",
    "    negative_counts_map = {ng: counts.get(ng, 1) for ng in unique_negatives}\n",
    "    \n",
    "    # Group negatives by count ranges (log-scale bins)\n",
    "    from collections import defaultdict\n",
    "    count_bins = defaultdict(list)\n",
    "    for ng in unique_negatives:\n",
    "        count = negative_counts_map[ng]\n",
    "        bin_key = int(np.log10(max(count, 1)))  # log10 binning\n",
    "        count_bins[bin_key].append(ng)\n",
    "    \n",
    "    bin_keys = list(count_bins.keys())\n",
    "    \n",
    "    print(\"Creating train set...\")\n",
    "    train_data = []\n",
    "    \n",
    "    for idx in tqdm(train_indices):\n",
    "        row = ngrammed_dataset[int(idx)]\n",
    "        query = row['text']\n",
    "        pos_chunks = row['pos_chunks']\n",
    "        \n",
    "        # Filter valid positives\n",
    "        pos_set = set(pos_chunks)\n",
    "        valid_positives = [p for p in pos_chunks if p not in eval_positives]\n",
    "        \n",
    "        if not valid_positives:\n",
    "            continue\n",
    "        \n",
    "        # Compute target count range for negatives based on positives\n",
    "        pos_counts = [counts.get(p, 1) for p in valid_positives]\n",
    "        target_bin = int(np.log10(max(np.mean(pos_counts), 1)))\n",
    "        \n",
    "        # Sample negatives from similar count bins\n",
    "        n_needed = len(valid_positives) * negatives_per_positive\n",
    "        negatives = []\n",
    "        \n",
    "        # Try target bin first, then expand to nearby bins\n",
    "        search_bins = [target_bin]\n",
    "        for offset in range(1, len(bin_keys) + 1):\n",
    "            if target_bin + offset in bin_keys:\n",
    "                search_bins.append(target_bin + offset)\n",
    "            if target_bin - offset in bin_keys:\n",
    "                search_bins.append(target_bin - offset)\n",
    "        \n",
    "        for bin_key in search_bins:\n",
    "            if len(negatives) >= n_needed:\n",
    "                break\n",
    "            \n",
    "            candidates = count_bins[bin_key]\n",
    "            random.shuffle(candidates)\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                if candidate not in pos_set:\n",
    "                    negatives.append(candidate)\n",
    "                    if len(negatives) >= n_needed:\n",
    "                        break\n",
    "        \n",
    "        # Fallback: if not enough negatives, sample from any bin\n",
    "        if len(negatives) < n_needed:\n",
    "            attempts = 0\n",
    "            max_attempts = n_needed * 10\n",
    "            while len(negatives) < n_needed and attempts < max_attempts:\n",
    "                candidate = random.choice(unique_negatives)\n",
    "                if candidate not in pos_set and candidate not in negatives:\n",
    "                    negatives.append(candidate)\n",
    "                attempts += 1\n",
    "        \n",
    "        # Create examples\n",
    "        for i, pos in enumerate(random.sample(valid_positives, k=min(experquery, len(valid_positives)))):\n",
    "            for j in range(min(negatives_per_positive, len(negatives) - i * negatives_per_positive)):\n",
    "                if i * negatives_per_positive + j < len(negatives):\n",
    "                    train_data.append({\n",
    "                        'query': query,\n",
    "                        'positive': pos,\n",
    "                        'negative': negatives[i * negatives_per_positive + j]\n",
    "                    })\n",
    "    \n",
    "    # Create eval set\n",
    "    print(\"Creating eval set...\")\n",
    "    eval_data = [\n",
    "        {\n",
    "            'question': ngrammed_dataset[int(idx)]['text'],\n",
    "            'pos_chunks': ngrammed_dataset[int(idx)]['pos_chunks']\n",
    "        }\n",
    "        for idx in eval_indices\n",
    "    ]\n",
    "    \n",
    "    # Print statistics\n",
    "    if train_data:\n",
    "        pos_counts = [counts.get(d['positive'], 1) for d in train_data]\n",
    "        neg_counts = [counts.get(d['negative'], 1) for d in train_data]\n",
    "        print(f\"\\nTrain set statistics:\")\n",
    "        print(f\"Average positive count: {np.mean(pos_counts):.2f} (median: {np.median(pos_counts):.2f})\")\n",
    "        print(f\"Average negative count: {np.mean(neg_counts):.2f} (median: {np.median(neg_counts):.2f})\")\n",
    "\n",
    "    # print number of positives from the test set that are in the train test\n",
    "    test_positives = set(eval_positives)\n",
    "    train_positives = set([r['positive'] for r in train_data])\n",
    "    print(len(test_positives & train_positives))\n",
    "    \n",
    "    return Dataset.from_list(train_data), Dataset.from_list(eval_data)\n",
    "\n",
    "# Usage\n",
    "tsamps = 50000\n",
    "train_eval = create_train_eval_splits_ultra_fast(\n",
    "    ngrammed_wikidocs, \n",
    "    counts,\n",
    "    train_ratio=0.99, \n",
    "    train_samples=tsamps, \n",
    "    negatives_per_positive=1, \n",
    "    seed=42, \n",
    "    max_negatives_pool=40000\n",
    ")\n",
    "train_ds, eval_ds = train_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27cdf6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating negative pool...\n",
      "Creating count bins for balanced sampling...\n",
      "Creating train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49500/49500 [00:06<00:00, 7970.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating eval set...\n",
      "\n",
      "Train set statistics:\n",
      "Average positive count: 20504.40 (median: 956.00)\n",
      "Average negative count: 19973.60 (median: 13571.00)\n",
      "2815\n"
     ]
    }
   ],
   "source": [
    "def create_train_eval_splits_ultra_fast_contaminated(ngrammed_dataset, counts, train_ratio=0.8, experquery=1, negatives_per_positive=1, seed=42, train_samples=50000, max_negatives_pool=100000):\n",
    "    \"\"\"\n",
    "    Ultra-fast version with balanced count sampling for negatives.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n_samples = train_samples\n",
    "    all_indices = np.arange(n_samples)\n",
    "    np.random.shuffle(all_indices)\n",
    "    \n",
    "    split_point = int(n_samples * train_ratio)\n",
    "    train_indices = all_indices[:split_point]\n",
    "    eval_indices = all_indices[split_point:]\n",
    "    \n",
    "    # Create a large pool of potential negatives (sampled once)\n",
    "    print(\"Creating negative pool...\")\n",
    "    all_ngrams = []\n",
    "    for row in ngrammed_dataset['pos_chunks']:\n",
    "        all_ngrams.extend(row)\n",
    "    \n",
    "    # Sample a subset for faster operations\n",
    "    if len(all_ngrams) > max_negatives_pool:\n",
    "        negative_pool = random.sample(all_ngrams, max_negatives_pool)\n",
    "    else:\n",
    "        negative_pool = all_ngrams\n",
    "    \n",
    "    # Create count-based bins for stratified sampling\n",
    "    print(\"Creating count bins for balanced sampling...\")\n",
    "    unique_negatives = list(set(negative_pool))\n",
    "    negative_counts_map = {ng: counts.get(ng, 1) for ng in unique_negatives}\n",
    "    \n",
    "    # Group negatives by count ranges (log-scale bins)\n",
    "    from collections import defaultdict\n",
    "    count_bins = defaultdict(list)\n",
    "    for ng in unique_negatives:\n",
    "        count = negative_counts_map[ng]\n",
    "        bin_key = int(np.log10(max(count, 1)))  # log10 binning\n",
    "        count_bins[bin_key].append(ng)\n",
    "    \n",
    "    bin_keys = list(count_bins.keys())\n",
    "    \n",
    "    print(\"Creating train set...\")\n",
    "    train_data = []\n",
    "    \n",
    "    for idx in tqdm(train_indices):\n",
    "        row = ngrammed_dataset[int(idx)]\n",
    "        query = row['text']\n",
    "        pos_chunks = row['pos_chunks']\n",
    "        \n",
    "        # Use all positives (no filtering)\n",
    "        pos_set = set(pos_chunks)\n",
    "        valid_positives = pos_chunks\n",
    "        \n",
    "        if not valid_positives:\n",
    "            continue\n",
    "        \n",
    "        # Compute target count range for negatives based on positives\n",
    "        pos_counts = [counts.get(p, 1) for p in valid_positives]\n",
    "        target_bin = int(np.log10(max(np.mean(pos_counts), 1)))\n",
    "        \n",
    "        # Sample negatives from similar count bins\n",
    "        n_needed = len(valid_positives) * negatives_per_positive\n",
    "        negatives = []\n",
    "        \n",
    "        # Try target bin first, then expand to nearby bins\n",
    "        search_bins = [target_bin]\n",
    "        for offset in range(1, len(bin_keys) + 1):\n",
    "            if target_bin + offset in bin_keys:\n",
    "                search_bins.append(target_bin + offset)\n",
    "            if target_bin - offset in bin_keys:\n",
    "                search_bins.append(target_bin - offset)\n",
    "        \n",
    "        for bin_key in search_bins:\n",
    "            if len(negatives) >= n_needed:\n",
    "                break\n",
    "            \n",
    "            candidates = count_bins[bin_key]\n",
    "            random.shuffle(candidates)\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                if candidate not in pos_set:\n",
    "                    negatives.append(candidate)\n",
    "                    if len(negatives) >= n_needed:\n",
    "                        break\n",
    "        \n",
    "        # Fallback: if not enough negatives, sample from any bin\n",
    "        if len(negatives) < n_needed:\n",
    "            attempts = 0\n",
    "            max_attempts = n_needed * 10\n",
    "            while len(negatives) < n_needed and attempts < max_attempts:\n",
    "                candidate = random.choice(unique_negatives)\n",
    "                if candidate not in pos_set and candidate not in negatives:\n",
    "                    negatives.append(candidate)\n",
    "                attempts += 1\n",
    "        \n",
    "        # Create examples\n",
    "        for i, pos in enumerate(random.sample(valid_positives, k=min(experquery, len(valid_positives)))):\n",
    "            for j in range(min(negatives_per_positive, len(negatives) - i * negatives_per_positive)):\n",
    "                if i * negatives_per_positive + j < len(negatives):\n",
    "                    train_data.append({\n",
    "                        'query': query,\n",
    "                        'positive': pos,\n",
    "                        'negative': negatives[i * negatives_per_positive + j]\n",
    "                    })\n",
    "    \n",
    "    # Create eval set\n",
    "    print(\"Creating eval set...\")\n",
    "    eval_data = [\n",
    "        {\n",
    "            'question': ngrammed_dataset[int(idx)]['text'],\n",
    "            'pos_chunks': ngrammed_dataset[int(idx)]['pos_chunks']\n",
    "        }\n",
    "        for idx in eval_indices\n",
    "    ]\n",
    "    \n",
    "    # Print statistics\n",
    "    if train_data:\n",
    "        pos_counts = [counts.get(d['positive'], 1) for d in train_data]\n",
    "        neg_counts = [counts.get(d['negative'], 1) for d in train_data]\n",
    "        print(f\"\\nTrain set statistics:\")\n",
    "        print(f\"Average positive count: {np.mean(pos_counts):.2f} (median: {np.median(pos_counts):.2f})\")\n",
    "        print(f\"Average negative count: {np.mean(neg_counts):.2f} (median: {np.median(neg_counts):.2f})\")\n",
    "\n",
    "    # print how much overlap there is between train and eval positives\n",
    "    train_positives = set([r['positive'] for r in train_data])\n",
    "    eval_positives = set([item for sublist in [r['pos_chunks'] for r in eval_data] for item in sublist])\n",
    "    print(len(train_positives & eval_positives))\n",
    "    \n",
    "    return Dataset.from_list(train_data), Dataset.from_list(eval_data)\n",
    "\n",
    "# Usage\n",
    "tsamps = 50000\n",
    "train_eval = create_train_eval_splits_ultra_fast_contaminated(\n",
    "    ngrammed_wikidocs, \n",
    "    counts,\n",
    "    train_ratio=0.99, \n",
    "    train_samples=tsamps, \n",
    "    negatives_per_positive=1, \n",
    "    seed=42, \n",
    "    max_negatives_pool=40000\n",
    ")\n",
    "train_ds, eval_ds = train_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89108fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contam = True\n",
    "if contam:\n",
    "    suff = \"balancedcontam\"\n",
    "else:\n",
    "    suff = \"balanced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd262557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 48500/48500 [00:00<00:00, 454674.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 180322.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 500/500 [00:00<00:00, 87217.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = DatasetDict({\n",
    "    'train': train_ds.select(range(1000, len(train_ds))),\n",
    "    'test': train_ds.select(range(1000))\n",
    "})\n",
    "train_ds.save_to_disk(f\"../propercache/data/colbert_training/wiki{n}gramtrain{tsamps}sample{suff}\")\n",
    "eval_ds.save_to_disk(f\"../propercache/data/evalsets/evalwiki{n}grameval{tsamps}samples{suff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed1d765a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 7390.85it/s]\n",
      "Map (num_proc=1): 100%|██████████| 62598/62598 [00:09<00:00, 6279.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4827 11026 45173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 50000/50000 [00:00<00:00, 2785099.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def dsets_to_dstore(n, samps, angs=None, dssize=50000):\n",
    "    traindata = DatasetDict.load_from_disk(f\"../propercache/data/colbert_training/wiki{n}gramtrain{samps}sample{suff}\")['train']\n",
    "    evaldata = Dataset.load_from_disk(f\"../propercache/data/evalsets/evalwiki{n}grameval{samps}samples{suff}\")\n",
    "\n",
    "    # get all pos_chuhnks from evaldata\n",
    "    allpos = [r['pos_chunks'] for r in evaldata]\n",
    "    allpos = list(set([item for sublist in allpos for item in sublist]))\n",
    "    # get all positives from traindata\n",
    "    allpos_train = list(set([r['positive'] for r in traindata]))\n",
    "    remaining = dssize - len(allpos)\n",
    "    print(len(allpos), len(allpos_train), remaining)\n",
    "    assert remaining > 0\n",
    "    if angs is not None:\n",
    "        aset = set(angs)\n",
    "        aset = list(aset - set(allpos))\n",
    "    else:\n",
    "        aset = list(set(allpos_train) - set(allpos))\n",
    "    usepos = random.sample(aset, k=remaining)\n",
    "    tdata = Dataset.from_list([{'text': p} for p in usepos + allpos])\n",
    "    return tdata\n",
    "\n",
    "n = 1\n",
    "chunked_wikidocs = chunk_ds(wikidocs, 10)\n",
    "ngrammed_wikidocs = chunked_wikidocs.map(lambda x: process_ngrams(x, n), num_proc=1)\n",
    "allngs_flat = [n for ng in ngrammed_wikidocs['pos_chunks'] for n in ng]\n",
    "counts = Counter(allngs_flat)\n",
    "dscount=50000\n",
    "tdstore = dsets_to_dstore(n, dscount, allngs_flat)\n",
    "tdstore.save_to_disk(f\"../propercache/data/datastores/wiki{n}gramdstore{dscount}{suff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eb46910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526235"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1484cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in stuff for inverted task\n",
    "t100krand = DatasetDict.load_from_disk(\"../propercache/data/colbert_training/nountrain100000minimal10dwords/\")\n",
    "testdstpre = Dataset.load_from_disk(\"../propercache/data/datastores/evaltdstore10words50pos100k\")\n",
    "testset = Dataset.load_from_disk(\"../propercache/data/evalsets/testset10words50pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb691b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctxt = set(testdstpre['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([\" \" in row['text'] for row in nouns100k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee3d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # do just once to avoid extra I/O\n",
    "    all_nouns = [word for synset in wn.all_synsets('n') for word in synset.lemma_names()]\n",
    "    all_nouns = [word for word in all_nouns if \"_\" not in word]\n",
    "    all_nouns = list(set(all_nouns))\n",
    "    random.shuffle(all_nouns)\n",
    "    test_nouns = [{'text': word} for word in all_nouns[:1000]]\n",
    "    # allnouns = Dataset.from_list([{\"text\": word} for word in all_nouns])\n",
    "    # allnouns.save_to_disk(\"../propercache/data/datastores/allnouns\")\n",
    "    nouns100k = Dataset.from_list([{\"text\": word} for word in all_nouns[1000:101000]])\n",
    "    nouns10k = Dataset.from_list([{\"text\": word} for word in all_nouns[1000:11000]])\n",
    "    Dataset.from_list(test_nouns).save_to_disk(\"../propercache/data/datastores/heldoutnouns\")\n",
    "    nouns100k.save_to_disk(\"../propercache/data/datastores/nouns100k\")\n",
    "    nouns10k.save_to_disk(\"../propercache/data/datastores/nouns10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98bffe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nouns = Dataset.load_from_disk(\"../propercache/data/datastores/heldoutnouns\")\n",
    "nouns100k = Dataset.load_from_disk(\"../propercache/data/datastores/nouns100k\")\n",
    "nouns10k = Dataset.load_from_disk(\"../propercache/data/datastores/nouns10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WE WILL HAVE 2 KINDS OF TASKS\n",
    "# A: Given a query (one word), return a list of chunks (each chunk has several words)\n",
    "# B: Given a query (a list of words), return a list of chunks (each chunk is just one noun at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0861ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOW STARTING SETUP FOR A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7821831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:08<00:00, 11400.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 11743.36it/s]\n",
      "Saving the dataset (2/2 shards): 100%|██████████| 100000/100000 [00:07<00:00, 13674.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 12090.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_train_query_rand(wset, doc_words=10):\n",
    "    query = random.choice(wset)['text']\n",
    "    chunkspos = [r['text'] for r in random.choices(wset, k=doc_words)]\n",
    "    \n",
    "    # Faster check and swap\n",
    "    if query not in chunkspos:\n",
    "        chunkspos[random.randint(0, doc_words-1)] = query\n",
    "    \n",
    "    chunksneg = [r['text'] for r in random.choices(wset, k=doc_words)]\n",
    "    \n",
    "    # Join once at the end\n",
    "    return {'query': query, 'positive': \" \".join(chunkspos), 'negative': \" \".join(chunksneg)}\n",
    "\n",
    "def generate_train_query_minimal(wset, doc_words=10):\n",
    "    query = random.choice(wset)['text']\n",
    "    \n",
    "    # Use random.sample directly instead of converting to set\n",
    "    chunks = [r['text'] for r in random.sample(wset, k=doc_words)]\n",
    "    \n",
    "    # Check and replace if needed\n",
    "    try:\n",
    "        idx = chunks.index(query)\n",
    "        chunks[idx] = random.choice(wset)['text']\n",
    "    except ValueError:\n",
    "        pass  # query not in chunks, which is fine\n",
    "    \n",
    "    negdata = \" \".join(chunks)\n",
    "    \n",
    "    # Swap for positive\n",
    "    chunks[random.randint(0, doc_words-1)] = query\n",
    "    posdata = \" \".join(chunks)\n",
    "    \n",
    "    return {'query': query, 'positive': posdata, 'negative': negdata}\n",
    "\n",
    "\n",
    "# Batch generation function - much faster\n",
    "def generate_batch_rand(wset_texts, count, doc_words=10):\n",
    "    results = []\n",
    "    for _ in tqdm(range(count)):\n",
    "        query = random.choice(wset_texts)\n",
    "        chunkspos = random.choices(wset_texts, k=doc_words)\n",
    "        \n",
    "        if query not in chunkspos:\n",
    "            chunkspos[random.randint(0, doc_words-1)] = query\n",
    "        \n",
    "        chunksneg = random.choices(wset_texts, k=doc_words)\n",
    "        \n",
    "        results.append({\n",
    "            'query': query, \n",
    "            'positive': \" \".join(chunkspos), \n",
    "            'negative': \" \".join(chunksneg)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Generate in larger batches\n",
    "dwords = 450\n",
    "datapoints = 100000\n",
    "\n",
    "\n",
    "# Pre-extract text once to avoid repeated dictionary access\n",
    "nouns10k_texts = [item['text'] for item in nouns10k]\n",
    "print(\"Generating training data...\")\n",
    "train_data = generate_batch_rand(nouns10k_texts, datapoints, dwords)\n",
    "print(\"Generating test data...\")\n",
    "test_data = generate_batch_rand(nouns10k_texts, 1000, dwords)\n",
    "\n",
    "train100krand = DatasetDict({\n",
    "    'train': Dataset.from_list(train_data),\n",
    "    'test': Dataset.from_list(test_data)\n",
    "})\n",
    "do_save=True\n",
    "if do_save:\n",
    "    train100krand.save_to_disk(f\"../propercache/data/colbert_training/v2nountrain{datapoints}rand{dwords}dwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5caa6c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'infelicity',\n",
       " 'positive': 'group toll Coregonus personableness endodontia sheeprun Bengali exfoliation Mylodontidae baklava Stalino Mb self-control encephalitis cadency Megaderma Rallidae ticktacktoo silicide honourableness pyorrhoea astrologer archosaurian pull-off Sabahan bursitis idiolect Pteris submucosa thirty-something shortia tutti-frutti amenia greenockite extrapolation tebibyte spine bagful smile tarot acclaim poking chocolate chondrite Hesperis ceramicist Uma apotheosis Espana mind Cynara tympanoplasty Struthio Rasmussen restaurateur sacque farad jacket infelicity Verona monody paynim Helminthostachys iodine nada Oct duffle Qadhafi microsome conoid Haggada equitation Heloise Vila landman boysenberry 4 burdock variety Saame Chlorophthalmidae ornithomimid sinciput Gustavus imbiber reservation Hera dissimilitude vulvectomy qat Papio Vincetoxicum Davys standardization asthma dish manufactory Amphisbaena epanalepsis detachment Dermochelys lattice picaninny Magyarorszag Lophiidae defroster sycophant numerousness bunkmate ANO double-decker fatalist dockworker Laredo harvestman Southwestern yellowhammer backlog Borodino Pisonia kenogenesis ECM diestrus rerun saddlery chanar Chalons-sur-Marne bouquet Sandburg Spheniscidae salvor Dermochelyidae puncher Postum Mithras wrester caroche garnishment Mutillidae peppermint multivalency anorthography cyclopropane parcelling traditionalism refocusing Colymbiformes harvestman Gaza dhak creeps gens aegir warp profanity hydrocracking mama Wright allyl Pycnogonida promulgation saxophone quadruped narcoterrorism DPRK sheikdom Stoppard docket hemiparasite benignity gunman finalization triton urbanisation ascendent syzygy anthelminthic Rambouillet Balaenoptera neurogliacyte ECG acclaim hypochondriac megabit crampoon Methodist manoeuvre Penn sledge Philippi graduality Sita varus Solenogastres uterus Paderewski winding-sheet announcement gentamicin indefatigability ravigote flowering thirty-something signaler gore spirograph colouring Ansaid chassis bard microsome cupping wiz turps Americanism pearl-fish isinglass stair-rod woodenness Eliomys paintbrush capitalisation greenery Fistularia goosefoot pearler Pocatello Stengel condor Hippocampus percentile squish anatomy canavanine palaeobotany Oreamnos Torricelli subversive Macbeth mundaneness gimpiness Lu pegleg hardwood ventilator leftover Dematiaceae Inanna Allen AFISR coltan christella spermatocide Palestine Winnebago sandpiper mistress Roumania palmature lats Synagrops assiduousness behaviorism Aureolaria creditworthiness neurogenesis world-weariness ameba rabato Bartlett algorithm piaster hematocyturia Lycaena combo Abyla southernism whydah SLS Pteretis villa relegating quarrelsomeness overtime semiconductor OK Passover Mauritian Photoblepharon Saxe-Coburg-Gotha alphabetizer Kore temperament tactlessness dasymeter Trachipteridae laetrile dogie pen-friend uplift fawn misreading antifeminist pinochle scurf Mansfield overage gooseflesh collegian Botox bps mayhem miscreation vaquero palpitation spoonbill youngness Proterozoic Dunkers Ful immatureness Lycopodineae cumquat mare saliency Muharrum presentism parachutist dieback conditionality Silverstein furane 1-dodecanol camphor transposability pipet Mbabane Palestine Rebekah foregrounding acolyte flammability muskrat Phalaropidae thimbleberry maiger highroad schtikl afterworld heftiness bridoon Morgantown copyhold Irani ensign yeshiva day Davenport shuttle hornblende radio Burlington Cretan psychic microprocessor mountaineering blogger sauciness surfboarding confab coolness aeromechanics soaring shortstop Karnataka Tabernacles octoroon peacekeeper feria hydroponics kaolinite whidah Panama candlepins rephrasing dreg plowman undertow salinity shearing conspirator Perseus flatulency turgidness vista Pedipalpi Karpov decompressing Trappist bourn tone Dionysius cityscape tux jaw tit-tat-toe denudation Mazdaism Hijaz idiot bacteriology Sihasapa hamper scrim wherefore householder collimation neurilemoma theocracy Musaceae tact comic NE turps shawny chairman pleasing superfamily hurdler billion numeracy fly-fishing mustang steward hatchery magic GBU-28 semantics Paraguay Haute-Normandie stanza personalty Lardner orbiter Peramelidae realism Wellington striptease Thoth grumble',\n",
       " 'negative': \"BVD's welfare violin faltering stolidity stipule Proverbs Han-Gook Formicidae Pesah Sardinian pintle desolation ensign Orizaba Beatitude summerset heterogeneousness Medicare pictograph earthtongue seminarian princess Sophonias characin Cortes dugout tom-tom mundaneness penultimate fitness non-Catholic overreckoning chaperone initiation Laccopetalum coral-wood creatine Persia formalin gonion ornithomimid derailment Camorra Thaddaeus MDMA epigenesis endogen drippings caesarian postcava antbird Corday tact Ephippidae smidgin avulsion turgidness breech trapping delinquent Eucharist Aphyllophorales corditis tympan disdainfulness Lander nub vandalism vinaigrette sprocket plastic southward Danube Halocarpus parathion inscription impotency shearing pigtail saviour gunsmith posterboard pre-emptor weapon roseola furniture ECM reinforcer brushup Cortez acetophenetidin mending strain stealth Hogarth Lateran lummox Spirochaetales streusel bearer Dendraspis slavery capstone nosy-parker OIG troposphere location tetterwort Leucadendron renewal mope body-build supplejack brazil matador Serranus Tuscarora sporran babu verifier Abronia indexing reprint Trondheim nitrocotton gainfulness Converso Platycephalidae nuisance accident Qatari canton Vila albuterol miaou diestrus eyecup sociability blackdamp corkage cardiospasm Australasia overtime rhombohedron deconstruction by-blow vexation cartridge 1-dodecanol presentism Piscidia cruet diddlyshit slipperiness hymnary metonymy pteridosperm purau tingle cocotte Leiden ichthyology Fouquieria provincialism flap profiling half-slip nardoo arcade stocktaking decompression whalebone Scirpus cisterna mullet yarn Katsuwonidae Badlands seek Mycetophilidae workbox indoctrination heterosexuality Mocambique dhak Dacron liveryman college insinuation reactor Vandyke miracle-worship riskiness biopsy Luscinia almsgiver bizarreness buckwheat octroi chopine unacceptability hustings laurels bud steatopygia baronduki insert microprocessor dredger oeuvre pollution refinisher gainfulness steradian transistor ring solitudinarian burgundy Trypetidae Gita clocks entrancement Methodist pitfall shirtdress belonging neoclassicist limey waif civilian isobar whortleberry lari Macrocheira yttrium Nederland Polk Loricata soreness galangal Sparta gulper Pisum Buchenwald flavour indestructibility Satanism killifish recounting halide noose revisal immatureness willowware celery gyp refinement uranium otorhinolaryngologist armor-bearer legislating Fermat tinter chateau lit wavelet cryptanalysis fly-fishing zoospore beaner cytoskeleton hatching Menander heftiness ironclad epiglottis latanier sexcapade Chapman filibuster astronavigation disforestation Wycherley tideway USPS ascetic Arno fedora jingoism pocketknife jumper Finnish freemasonry self-respect myogram pot Menelaus aquiculture hack Ramman lordship retirement amphibian guaranty renown heterosexuality church Ectopistes ratter Cyamopsis Callionymidae Sicily cassowary Dior scherzo Baikal heterotaxy Cryptotis cadency Emmy anthology styptic Palestine Patwin bread misuse omission macroeconomist voluminosity peperomia Hibbing action ecumenism Furnivall Ummah litmus windbag label iguanid galago daydream Malvaviscus onychophoran verifier quira ability ratiocination provost sculpin fraise bailor exfoliation pyuria polyoma acetaldehyde xylol Hunkpapa counterbore DA biochip benefaction Valparaiso hovercraft Spode placental manufacture centaury suffusion spray homebrew armament acarophobia carrizo spreadsheet centrality raiser cognition doormat Stoppard vallecula Haeckel walker stowage rationale acoustic Actinomyxidia caraway frazzle candidature industrialist cock Alps Leucadendron regatta arsenate Gopher rightfield vacuity nosebleed reveller osprey Justice corpse milium pickaxe mortgage kipper rattlebox echinocactus NV witnesser Hypentelium sommelier Jell-O muff suntrap dependent Hindoostani Chernobyl gipsy performance erythrocyte nitrobacteria welkin sluice Anasa yawn merit Sicily authoritarianism Claviceps deer's-ears hyperemia teg digestibleness armor-bearer mudcat Khanty underpart Vermont caustic indefatigableness winemaker Lophophora thermometry indecision abnegation Rastafarian beaner\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train100krand['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99da0c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'infelicity',\n",
       " 'positive': 'group toll Coregonus personableness endodontia sheeprun Bengali exfoliation Mylodontidae baklava Stalino Mb self-control encephalitis cadency Megaderma Rallidae ticktacktoo silicide honourableness pyorrhoea astrologer archosaurian pull-off Sabahan bursitis idiolect Pteris submucosa thirty-something shortia tutti-frutti amenia greenockite extrapolation tebibyte spine bagful smile tarot acclaim poking chocolate chondrite Hesperis ceramicist Uma apotheosis Espana mind Cynara tympanoplasty Struthio Rasmussen restaurateur sacque farad jacket infelicity Verona monody paynim Helminthostachys iodine nada Oct duffle Qadhafi microsome conoid Haggada equitation Heloise Vila landman boysenberry 4 burdock variety Saame Chlorophthalmidae ornithomimid sinciput Gustavus imbiber reservation Hera dissimilitude vulvectomy qat Papio Vincetoxicum Davys standardization asthma dish manufactory Amphisbaena epanalepsis detachment Dermochelys lattice picaninny Magyarorszag Lophiidae defroster sycophant numerousness bunkmate ANO double-decker fatalist dockworker Laredo harvestman Southwestern yellowhammer backlog Borodino Pisonia kenogenesis ECM diestrus rerun saddlery chanar Chalons-sur-Marne bouquet Sandburg Spheniscidae salvor Dermochelyidae puncher Postum Mithras wrester caroche garnishment Mutillidae peppermint multivalency anorthography cyclopropane parcelling traditionalism refocusing Colymbiformes harvestman Gaza dhak creeps gens aegir warp profanity hydrocracking mama Wright allyl Pycnogonida promulgation saxophone quadruped narcoterrorism DPRK sheikdom Stoppard docket hemiparasite benignity gunman finalization triton urbanisation ascendent syzygy anthelminthic Rambouillet Balaenoptera neurogliacyte ECG acclaim hypochondriac megabit crampoon Methodist manoeuvre Penn sledge Philippi graduality Sita varus Solenogastres uterus Paderewski winding-sheet announcement gentamicin indefatigability ravigote flowering thirty-something signaler gore spirograph colouring Ansaid chassis bard microsome cupping wiz turps Americanism pearl-fish isinglass stair-rod woodenness Eliomys paintbrush capitalisation greenery Fistularia goosefoot pearler Pocatello Stengel condor Hippocampus percentile squish anatomy canavanine palaeobotany Oreamnos Torricelli subversive Macbeth mundaneness gimpiness Lu pegleg hardwood ventilator leftover Dematiaceae Inanna Allen AFISR coltan christella spermatocide Palestine Winnebago sandpiper mistress Roumania palmature lats Synagrops assiduousness behaviorism Aureolaria creditworthiness neurogenesis world-weariness ameba rabato Bartlett algorithm piaster hematocyturia Lycaena combo Abyla southernism whydah SLS Pteretis villa relegating quarrelsomeness overtime semiconductor OK Passover Mauritian Photoblepharon Saxe-Coburg-Gotha alphabetizer Kore temperament tactlessness dasymeter Trachipteridae laetrile dogie pen-friend uplift fawn misreading antifeminist pinochle scurf Mansfield overage gooseflesh collegian Botox bps mayhem miscreation vaquero palpitation spoonbill youngness Proterozoic Dunkers Ful immatureness Lycopodineae cumquat mare saliency Muharrum presentism parachutist dieback conditionality Silverstein furane 1-dodecanol camphor transposability pipet Mbabane Palestine Rebekah foregrounding acolyte flammability muskrat Phalaropidae thimbleberry maiger highroad schtikl afterworld heftiness bridoon Morgantown copyhold Irani ensign yeshiva day Davenport shuttle hornblende radio Burlington Cretan psychic microprocessor mountaineering blogger sauciness surfboarding confab coolness aeromechanics soaring shortstop Karnataka Tabernacles octoroon peacekeeper feria hydroponics kaolinite whidah Panama candlepins rephrasing dreg plowman undertow salinity shearing conspirator Perseus flatulency turgidness vista Pedipalpi Karpov decompressing Trappist bourn tone Dionysius cityscape tux jaw tit-tat-toe denudation Mazdaism Hijaz idiot bacteriology Sihasapa hamper scrim wherefore householder collimation neurilemoma theocracy Musaceae tact comic NE turps shawny chairman pleasing superfamily hurdler billion numeracy fly-fishing mustang steward hatchery magic GBU-28 semantics Paraguay Haute-Normandie stanza personalty Lardner orbiter Peramelidae realism Wellington striptease Thoth grumble',\n",
       " 'negative': \"BVD's welfare violin faltering stolidity stipule Proverbs Han-Gook Formicidae Pesah Sardinian pintle desolation ensign Orizaba Beatitude summerset heterogeneousness Medicare pictograph earthtongue seminarian princess Sophonias characin Cortes dugout tom-tom mundaneness penultimate fitness non-Catholic overreckoning chaperone initiation Laccopetalum coral-wood creatine Persia formalin gonion ornithomimid derailment Camorra Thaddaeus MDMA epigenesis endogen drippings caesarian postcava antbird Corday tact Ephippidae smidgin avulsion turgidness breech trapping delinquent Eucharist Aphyllophorales corditis tympan disdainfulness Lander nub vandalism vinaigrette sprocket plastic southward Danube Halocarpus parathion inscription impotency shearing pigtail saviour gunsmith posterboard pre-emptor weapon roseola furniture ECM reinforcer brushup Cortez acetophenetidin mending strain stealth Hogarth Lateran lummox Spirochaetales streusel bearer Dendraspis slavery capstone nosy-parker OIG troposphere location tetterwort Leucadendron renewal mope body-build supplejack brazil matador Serranus Tuscarora sporran babu verifier Abronia indexing reprint Trondheim nitrocotton gainfulness Converso Platycephalidae nuisance accident Qatari canton Vila albuterol miaou diestrus eyecup sociability blackdamp corkage cardiospasm Australasia overtime rhombohedron deconstruction by-blow vexation cartridge 1-dodecanol presentism Piscidia cruet diddlyshit slipperiness hymnary metonymy pteridosperm purau tingle cocotte Leiden ichthyology Fouquieria provincialism flap profiling half-slip nardoo arcade stocktaking decompression whalebone Scirpus cisterna mullet yarn Katsuwonidae Badlands seek Mycetophilidae workbox indoctrination heterosexuality Mocambique dhak Dacron liveryman college insinuation reactor Vandyke miracle-worship riskiness biopsy Luscinia almsgiver bizarreness buckwheat octroi chopine unacceptability hustings laurels bud steatopygia baronduki insert microprocessor dredger oeuvre pollution refinisher gainfulness steradian transistor ring solitudinarian burgundy Trypetidae Gita clocks entrancement Methodist pitfall shirtdress belonging neoclassicist limey waif civilian isobar whortleberry lari Macrocheira yttrium Nederland Polk Loricata soreness galangal Sparta gulper Pisum Buchenwald flavour indestructibility Satanism killifish recounting halide noose revisal immatureness willowware celery gyp refinement uranium otorhinolaryngologist armor-bearer legislating Fermat tinter chateau lit wavelet cryptanalysis fly-fishing zoospore beaner cytoskeleton hatching Menander heftiness ironclad epiglottis latanier sexcapade Chapman filibuster astronavigation disforestation Wycherley tideway USPS ascetic Arno fedora jingoism pocketknife jumper Finnish freemasonry self-respect myogram pot Menelaus aquiculture hack Ramman lordship retirement amphibian guaranty renown heterosexuality church Ectopistes ratter Cyamopsis Callionymidae Sicily cassowary Dior scherzo Baikal heterotaxy Cryptotis cadency Emmy anthology styptic Palestine Patwin bread misuse omission macroeconomist voluminosity peperomia Hibbing action ecumenism Furnivall Ummah litmus windbag label iguanid galago daydream Malvaviscus onychophoran verifier quira ability ratiocination provost sculpin fraise bailor exfoliation pyuria polyoma acetaldehyde xylol Hunkpapa counterbore DA biochip benefaction Valparaiso hovercraft Spode placental manufacture centaury suffusion spray homebrew armament acarophobia carrizo spreadsheet centrality raiser cognition doormat Stoppard vallecula Haeckel walker stowage rationale acoustic Actinomyxidia caraway frazzle candidature industrialist cock Alps Leucadendron regatta arsenate Gopher rightfield vacuity nosebleed reveller osprey Justice corpse milium pickaxe mortgage kipper rattlebox echinocactus NV witnesser Hypentelium sommelier Jell-O muff suntrap dependent Hindoostani Chernobyl gipsy performance erythrocyte nitrobacteria welkin sluice Anasa yawn merit Sicily authoritarianism Claviceps deer's-ears hyperemia teg digestibleness armor-bearer mudcat Khanty underpart Vermont caustic indefatigableness winemaker Lophophora thermometry indecision abnegation Rastafarian beaner\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train100krand['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f4a8388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:13<00:00, 7472.90it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 17833.53it/s]\n"
     ]
    }
   ],
   "source": [
    "def singwordquery_eval(evalnouns, trainnouns, dstoresize=10000, tsetsize=500, num_pos=50, docwords=10):\n",
    "    evsetnouns = set([r['text'] for r in evalnouns])\n",
    "    trsetnouns = set([r['text'] for r in trainnouns])\n",
    "    assert len(evsetnouns & trsetnouns) == 0\n",
    "    assert all([\" \" not in r['text'] for r in evalnouns])\n",
    "    assert all([\" \" not in r['text'] for r in trainnouns])\n",
    "    evalnouns = list([r['text'] for r in evalnouns])\n",
    "    trainnouns = list([r['text'] for r in trainnouns])\n",
    "    # make eval test set and datastore\n",
    "    testqueries = random.sample(evalnouns, k=tsetsize)\n",
    "    # make eval datastore, with each document have docwords words\n",
    "    starterdocs = [[r for r in random.sample(trainnouns, k=docwords)] for _ in tqdm(range(dstoresize))]\n",
    "    # for each query, randomly choose num_pos docs from starterdocs. For each of these randomly replace one word with query word\n",
    "    query_poschunks = []\n",
    "    indlist = list(range(dstoresize))\n",
    "    for query in tqdm(testqueries):\n",
    "        posinds = random.sample(indlist, k=num_pos)\n",
    "        for posind in posinds:\n",
    "            # randomly replace one word in starterdocs[posind] with query word\n",
    "            wordind = random.randint(0, docwords-1)\n",
    "            starterdocs[posind][wordind] = query\n",
    "        query_poschunks.append(posinds)\n",
    "    doc_poschunks = Dataset.from_list([{\"text\": \" \".join(d)} for d in starterdocs])\n",
    "    query_data = Dataset.from_list([\n",
    "        {\n",
    "            \"question\": q, \n",
    "            \"pos_chunks\": [doc_poschunks[p]['text'] for p in query_poschunks[i]], \n",
    "            \"num_pos_chunks\": len(query_poschunks[i])\n",
    "        } \n",
    "        for i, q in enumerate(testqueries)])\n",
    "    return doc_poschunks, query_data\n",
    "\n",
    "dwords = 450\n",
    "npos=50\n",
    "# get a test set to work with \n",
    "tdstore, testset10words50pos = singwordquery_eval(test_nouns, nouns10k, 100000, 500, npos, dwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64e20e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 100000/100000 [00:03<00:00, 27658.67 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 500/500 [00:00<00:00, 541.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tdstore.save_to_disk(f\"../propercache/data/datastores/v2evaltdstore{dwords}words{npos}pos100k\")\n",
    "testset10words50pos.save_to_disk(f\"../propercache/data/evalsets/v2testset{dwords}words{npos}pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tns = set([r['text'] for r in test_nouns])\n",
    "trainnns = set(nouns100k['text'])\n",
    "print(len(tns), len(trainnns), len(tns & trainnns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee8889",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdstore = Dataset.load_from_disk(\"../propercache/data/datastores/v2evaltdstore50words50pos100k\")\n",
    "testset10words50pos = Dataset.load_from_disk(\"../propercache/data/evalsets/v2testset50words50pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdstore = Dataset.load_from_disk(\"../propercache/data/datastores/v2evaltdstore50words50pos100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa580d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset10words50pos[8]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([row['text'] for row in tdstore if f\" tram \" in row['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc579e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcnts = []\n",
    "qs = list(testset10words50pos['question'])\n",
    "for q in tqdm(qs):\n",
    "    wordcnts.append(sum([f\" {q} \" in row['text'] for row in tdstore]))\n",
    "    print(wordcnts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4810946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdstore.save_to_disk(\"../propercache/data/datastores/evaltdstore10words50pos100k\")\n",
    "# testset10words50pos.save_to_disk(\"../propercache/data/evalsets/testset10words50pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make eval datastore (main thing is making datastore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapoints = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc69419",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOW STARTING SETUP FOR B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eebf1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_toy_query(wset, wset_texts, query_words=100, ndps=1, datatype=\"train\"):\n",
    "    querywords = random.choices(wset, k=query_words)\n",
    "    querywords_text = [q['text'] for q in querywords]\n",
    "    query = \" \".join(querywords_text)\n",
    "    poslist = random.choices(querywords_text, k=ndps)\n",
    "    \n",
    "    # Convert to set for O(1) lookup\n",
    "    querywords_set = set(querywords_text)\n",
    "    \n",
    "    # Sample negatives - just keep trying with random.choice until we get valid ones\n",
    "    neglist = []\n",
    "    for _ in range(ndps):\n",
    "        while True:\n",
    "            neg = random.choice(wset_texts)\n",
    "            if neg not in querywords_set:\n",
    "                neglist.append(neg)\n",
    "                break\n",
    "    \n",
    "    if datatype == \"train\":\n",
    "        return [{'query': query, 'positive': pos, 'negative': neg} \n",
    "                for pos, neg in zip(poslist, neglist)]\n",
    "    else:\n",
    "        return {'question': query, 'pos_chunks': querywords_text, \n",
    "                'numposchunks': len(querywords_text)}\n",
    "\n",
    "def toyquerydset(wset, qwords, ndps, tsize):\n",
    "    # Pre-extract all texts from wset ONCE\n",
    "    wset_texts = [w['text'] for w in wset]\n",
    "    \n",
    "    alldata = []\n",
    "    inddps = tsize // ndps\n",
    "    for _ in tqdm(range(inddps)):\n",
    "        alldata.extend(construct_toy_query(wset, wset_texts, qwords, ndps, \"train\"))\n",
    "    return Dataset.from_list(alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a23e02ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:38<00:00, 1017.98it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1015.25it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100000/100000 [00:00<00:00, 193094.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 114937.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "numwords = 50\n",
    "tset = 100000\n",
    "dp_per_query = 1\n",
    "traindata = DatasetDict({\n",
    "    'train': toyquerydset(nouns10k, numwords, dp_per_query, tset),\n",
    "    'test': toyquerydset(nouns10k, numwords, dp_per_query, 1000)\n",
    "})\n",
    "traindata.save_to_disk(f\"../propercache/data/colbert_training/v2nountraining{numwords}words{tset}ndps{dp_per_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc1dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET EVAL DATA\n",
    "evallists = [50] # [1, 5, 10, 25]\n",
    "tnountexts = [r['text'] for r in test_nouns]\n",
    "for ev in evallists:\n",
    "    evdata = Dataset.from_list([construct_toy_query(test_nouns, tnountexts, query_words=ev, datatype=\"eval\") for _ in tqdm(range(500))])\n",
    "    if False:\n",
    "        evdata.save_to_disk(f\"../propercache/data/evalsets/v2nountest{ev}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48702939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate eval data following a probability distribution over count of query words\n",
    "def pdquerydset(wset, qwdistdict, tsize, ndps=1):\n",
    "    alldata = []\n",
    "    dcnt = 0\n",
    "    while dcnt < tsize:\n",
    "        qw = random.choices(list(qwdistdict.keys()), weights=list(qwdistdict.values()), k=1)[0]\n",
    "        alldata.extend(construct_toy_query(wset, qw, min(ndps, qw), \"train\"))\n",
    "        dcnt += 1\n",
    "        if dcnt % 10000 == 0:\n",
    "            print(f\"Generated {dcnt} queries\")\n",
    "    return Dataset.from_list(alldata)\n",
    "\n",
    "# uniform distribution from 5 to 100\n",
    "qwdistdictuni = {i: 1/96 for i in range(5, 101)}\n",
    "# power law distribution from 5 to 100\n",
    "qwdistdictpower = {i: i**(-0.5) for i in range(5, 101)}\n",
    "\n",
    "# plot power law distribution in qwdistdictpower\n",
    "x = list(qwdistdictpower.keys())\n",
    "y = list(qwdistdictpower.values())\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Number of Words in Query\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Power Law Distribution of Query Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 100k train set for each distribution\n",
    "train100kuniform = pdquerydset(nouns10k, qwdistdictuni, 100000)\n",
    "test100kuniform = pdquerydset(nouns10k, qwdistdictuni, 1000)\n",
    "\n",
    "train100kpower = pdquerydset(nouns10k, qwdistdictpower, 100000)\n",
    "test100kpower = pdquerydset(nouns10k, qwdistdictpower, 1000)\n",
    "\n",
    "DatasetDict({\n",
    "    'train': train100kuniform,\n",
    "    'test': test100kuniform\n",
    "}).save_to_disk(\"../propercache/data/colbert_training/nountraining100kuniform5_100\")\n",
    "\n",
    "DatasetDict({\n",
    "    'train': train100kpower,\n",
    "    'test': test100kpower\n",
    "}).save_to_disk(\"../propercache/data/colbert_training/nountraining100kpower5_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9051b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check distributions of 'query' length in train100kuniform, train100kpower\n",
    "plt.hist([len(q['query'].split()) for q in train100kuniform], alpha=0.5, label='Uniform', range=(5, 100))\n",
    "plt.hist([len(q['query'].split()) for q in train100kpower], alpha=0.5, label='Power', range=(5, 100))\n",
    "plt.ylabel(\"Count\") \n",
    "plt.xlabel(\"Number of Words in Query\")\n",
    "plt.title(\"Distribution of Query Length in Training Sets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "evallists = [1, 5, 10, 25]\n",
    "testsets = {}\n",
    "for ev in evallists:\n",
    "    testsets[ev] = Dataset.load_from_disk(f\"../propercache/data/evalsets/nountest{ev}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "allnouns = Dataset.load_from_disk(\"../propercache/data/datastores/allnouns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa27ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ansfixed = list(set([a['text'] for a in allnouns]))\n",
    "ansfixed = Dataset.from_list([{'text': a} for a in ansfixed])\n",
    "ansfixed.save_to_disk(\"../propercache/data/datastores/allnounsfixed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb69efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = list([a['text'] for a in allnouns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29235147",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ans), len(set(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a test set, convert it to a train set\n",
    "def test_to_trainset(tset):\n",
    "    alldata = []\n",
    "    for row in tset:\n",
    "        for pos in row['pos_chunks']:\n",
    "            alldata.append({'query': row['question'], 'positive': pos, 'negative': random.choice(allnouns)['text']})\n",
    "    return DatasetDict({'train': Dataset.from_list(alldata), 'test': Dataset.from_list(alldata)})\n",
    "\n",
    "tset = 25\n",
    "test_to_trainset(testsets[tset]).save_to_disk(f\"../propercache/data/colbert_training/nountestset{tset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398076b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_trainset(testsets[tset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32066cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval10 = Dataset.from_list([construct_toy_query(test_nouns, 10, \"eval\") for _ in tqdm(range(1000))])\n",
    "eval100 = Dataset.from_list([construct_toy_query(test_nouns, 100, \"eval\") for _ in tqdm(range(1000))])\n",
    "\n",
    "eval10.save_to_disk(\"../propercache/data/evalsets/nountest10/\")\n",
    "eval100.save_to_disk(\"../propercache/data/evalsets/nountest100/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4352c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO need to make eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata['train'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c2e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaling3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
