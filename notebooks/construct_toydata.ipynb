{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "194b86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toydata to test simple algorithm\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "from setretrieval.utils.utils import pickload, pickdump\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38e698fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nouns = [word for synset in wn.all_synsets('n') for word in synset.lemma_names()]\n",
    "all_nouns = [word for word in all_nouns if \"_\" not in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39c5678d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83939"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5b0c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nouns = [{'text': word} for word in all_nouns[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f60b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 264191.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 82939/82939 [00:00<00:00, 2846435.15 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 1589835.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "nouns100k = Dataset.from_list([{\"text\": word} for word in all_nouns[1000:101000]])\n",
    "nouns10k = Dataset.from_list([{\"text\": word} for word in all_nouns[1000:11000]])\n",
    "test_nouns.save_to_disk(\"../propercache/data/datastores/heldoutnouns\")\n",
    "nouns100k.save_to_disk(\"../propercache/data/datastores/nouns100k\")\n",
    "nouns10k.save_to_disk(\"../propercache/data/datastores/nouns10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eebf1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare query, pos, negative data for this stuff\n",
    "def construct_toy_query(wset, query_words=100, datatype=\"train\"):\n",
    "    querywords = random.choices(wset, k=query_words)\n",
    "    querywords = [q['text'] for q in querywords]\n",
    "    query = \" \".join(querywords)\n",
    "    pos = random.choice(querywords)\n",
    "    while True: \n",
    "        neg = random.choice(wset)['text']\n",
    "        if neg not in querywords:\n",
    "            break\n",
    "    if datatype == \"train\":\n",
    "        return {'query': query, 'positive': pos, 'negative': neg}\n",
    "    else:\n",
    "        return {'question': query, 'pos_chunks': querywords, 'numposchunks': len(querywords)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66a08182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 83939/83939 [00:00<00:00, 3347013.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "allnouns = Dataset.from_list([{\"text\": word} for word in all_nouns])\n",
    "allnouns.save_to_disk(\"../propercache/data/datastores/allnouns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a23e02ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:18<00:00, 528.46it/s]\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 488.45it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 103539.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 61916.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "numwords = 100\n",
    "traindata = DatasetDict({\n",
    "    'train': Dataset.from_list([construct_toy_query(nouns10k, numwords) for _ in tqdm(range(10000))]),\n",
    "    'test': Dataset.from_list([construct_toy_query(nouns10k, numwords) for _ in tqdm(range(1000))])\n",
    "})\n",
    "traindata.save_to_disk(f\"../propercache/data/colbert_training/nountraining{numwords}words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32066cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 111708.10it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 29709.96it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 152348.41 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 43075.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval10 = Dataset.from_list([construct_toy_query(test_nouns, 10, \"eval\") for _ in tqdm(range(1000))])\n",
    "eval100 = Dataset.from_list([construct_toy_query(test_nouns, 100, \"eval\") for _ in tqdm(range(1000))])\n",
    "\n",
    "eval10.save_to_disk(\"../propercache/data/evalsets/nountest10/\")\n",
    "eval100.save_to_disk(\"../propercache/data/evalsets/nountest100/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4352c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO need to make eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "955d6800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'spoonfeeding plastron spare reforestation crapulence heave running involvement Dasyurus Torpediniformes travelling eco-warfare numismatology onslaught standardisation facelift shower quitclaim Gerrhonotus redbelly IPO Petromyzontidae Ophiophagus crocodile absenteeism angiography schematization aspiration penning secularisation egg follow-through despoliation transference disforestation historiography splurge plunge cryptomonad discipline soar infusion cockateel poll Arizona Fucales seigniory watching viceroyship retroversion nothosaur Actinia free-for-all passing bestowal trial Dicamptodontidae retribution name peckerwood circumvolution hybrid interference nationalisation hang justice break capture composing calf sifting interpenetration olm messiahship whip-snake operation diagramming prickleback Merostomata counting jerking expiration bacillus road regimentation specialization assignment conferva operations orchotomy enjoyment bust metazoan breaking sacrament figuration nookie pick robin sailing',\n",
       " 'pos': 'Gerrhonotus',\n",
       " 'neg': 'renewal'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata['train'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c2e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaling2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
