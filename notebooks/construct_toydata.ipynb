{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "194b86bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/projects/sewonm/prasann/.conda/envs/scaling2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# toydata to test simple algorithm\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "from setretrieval.utils.utils import pickload, pickdump\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e698fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nouns = [word for synset in wn.all_synsets('n') for word in synset.lemma_names()]\n",
    "all_nouns = [word for word in all_nouns if \"_\" not in word]\n",
    "test_nouns = [{'text': word} for word in all_nouns[:1000]]\n",
    "# allnouns = Dataset.from_list([{\"text\": word} for word in all_nouns])\n",
    "# allnouns.save_to_disk(\"../propercache/data/datastores/allnouns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee3d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns100k = Dataset.from_list([{\"text\": word} for word in all_nouns[1000:101000]])\n",
    "nouns10k = Dataset.from_list([{\"text\": word} for word in all_nouns[1000:11000]])\n",
    "if False: # do just once to avoid extra I/O\n",
    "    test_nouns.save_to_disk(\"../propercache/data/datastores/heldoutnouns\")\n",
    "    nouns100k.save_to_disk(\"../propercache/data/datastores/nouns100k\")\n",
    "    nouns10k.save_to_disk(\"../propercache/data/datastores/nouns10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WE WILL HAVE 2 KINDS OF TASKS\n",
    "# A: Given a query (one word), return a list of chunks (each chunk has several words)\n",
    "# B: Given a query (a list of words), return a list of chunks (each chunk is just one noun at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0861ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOW STARTING SETUP FOR A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b9044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 100000/100000 [00:22<00:00, 4354.07it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4256.82it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100000/100000 [00:00<00:00, 416193.66 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 164038.64 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100000/100000 [00:00<00:00, 413210.75 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 101798.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_train_query_rand(wset, doc_words=10):\n",
    "    query = random.choice(wset)['text']\n",
    "    chunkspos = [r['text'] for r in random.choices(wset, k=doc_words)]\n",
    "    # if query not in chunkspos, randomly swap one of the chunks with query\n",
    "    if query not in chunkspos:\n",
    "        swap_idx = random.randint(0, len(chunkspos)-1)\n",
    "        chunkspos[swap_idx] = query\n",
    "    chunksneg = [r['text'] for r in random.choices(wset, k=doc_words)]\n",
    "    return {'query': query, 'positive': \" \".join(chunkspos), 'negative': \" \".join(chunksneg)}\n",
    "\n",
    "# TODO this will help us test if minimality somehow makes data better for \"hard negatives\"\n",
    "# - Note this is pretty clean minimality, other stuff could / could not matter\n",
    "def generate_train_query_minimal(wset, doc_words=10):\n",
    "    query = random.choice(wset)['text']\n",
    "    chunks = set([r['text'] for r in random.choices(wset, k=doc_words)])\n",
    "    # TODO sanity check that some weird alphabetization isn't doing something weird\n",
    "    # make sure query is not in chunks\n",
    "    if query in chunks:\n",
    "        chunks.remove(query)\n",
    "        chunks.add(random.choice(wset)['text'])\n",
    "    negdata = \" \".join(chunks)\n",
    "    # now randomly swap one of the chunks with query\n",
    "    swap_idx = random.randint(0, len(chunks)-1)\n",
    "    chunks = list(chunks)\n",
    "    chunks[swap_idx] = query\n",
    "    posdata = \" \".join(chunks)\n",
    "    return {'query': query, 'positive': posdata, 'negative': negdata}\n",
    "\n",
    "# get 100k train sets for both kinds\n",
    "dwords = 10\n",
    "datapoints = 100000\n",
    "# train100krand = DatasetDict({\n",
    "#     'train': Dataset.from_list([generate_train_query_rand(nouns10k, dwords) for _ in tqdm(range(datapoints))]),\n",
    "#     'test': Dataset.from_list([generate_train_query_rand(nouns10k, dwords) for _ in tqdm(range(1000))])\n",
    "# })\n",
    "train100kminimal = DatasetDict({\n",
    "    'train': Dataset.from_list([generate_train_query_minimal(nouns10k, dwords) for _ in tqdm(range(datapoints))]),\n",
    "    'test': Dataset.from_list([generate_train_query_minimal(nouns10k, dwords) for _ in tqdm(range(1000))])\n",
    "})\n",
    "do_save = True\n",
    "if do_save:\n",
    "    train100krand.save_to_disk(f\"../propercache/data/colbert_training/nountrain{datapoints}rand{dwords}dwords\")\n",
    "    train100kminimal.save_to_disk(f\"../propercache/data/colbert_training/nountrain{datapoints}minimal{dwords}dwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e807580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'snapper',\n",
       " 'positive': 'Caprimulgidae Mycrosporidia snapper lovebird buffalofish athletics reconnaissance Orectolobus debunking ectoparasite',\n",
       " 'negative': 'Caprimulgidae Mycrosporidia rescission lovebird buffalofish athletics reconnaissance Orectolobus debunking ectoparasite'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train100kminimal['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f4a8388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 100000/100000 [00:21<00:00, 4671.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 500/500 [00:00<00:00, 877.74it/s]\n"
     ]
    }
   ],
   "source": [
    "def singwordquery_eval(evalnouns, trainnouns, dstoresize=100000, tsetsize=500, num_pos=50, docwords=10):\n",
    "    # make eval test set and datastore\n",
    "    testqueries = random.choices(evalnouns, k=tsetsize)\n",
    "    # make eval datastore, with each document have docwords words\n",
    "    starterdocs = [[r['text'] for r in random.choices(trainnouns, k=docwords)] for _ in tqdm(range(dstoresize))]\n",
    "    # for each query, randomly choose num_pos docs from starterdocs. For each of these randomly replace one word with query word\n",
    "    query_poschunks = []\n",
    "    for query in tqdm(testqueries):\n",
    "        posinds = random.choices(list(range(dstoresize)), k=num_pos)\n",
    "        for posind in posinds:\n",
    "            # randomly replace one word in starterdocs[posind] with query word\n",
    "            wordind = random.randint(0, docwords-1)\n",
    "            starterdocs[posind][wordind] = query['text']\n",
    "        query_poschunks.append(posinds)\n",
    "    doc_poschunks = Dataset.from_list([{\"text\": \" \".join(d)} for d in starterdocs])\n",
    "    query_data = Dataset.from_list([\n",
    "        {\n",
    "            \"question\": q, \n",
    "            \"pos_chunks\": [doc_poschunks[p]['text'] for p in query_poschunks[i]], \n",
    "            \"num_pos_chunks\": len(query_poschunks[i])\n",
    "        } \n",
    "        for i, q in enumerate(testqueries)])\n",
    "    return doc_poschunks, query_data\n",
    "\n",
    "# get a test set to work with \n",
    "tdstore, testset10words50pos = singwordquery_eval(test_nouns, nouns10k, 100000, 500, 50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4810946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100000/100000 [00:00<00:00, 835051.30 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 500/500 [00:00<00:00, 17993.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tdstore.save_to_disk(\"../propercache/data/datastores/evaltdstore10words50pos100k\")\n",
    "testset10words50pos.save_to_disk(\"../propercache/data/evalsets/testset10words50pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make eval datastore (main thing is making datastore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapoints = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc69419",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOW STARTING SETUP FOR B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare query, pos, negative data for this stuff\n",
    "def construct_toy_query(wset, query_words=100, ndps=1, datatype=\"train\"):\n",
    "    querywords = random.choices(wset, k=query_words)\n",
    "    querywords = [q['text'] for q in querywords]\n",
    "    query = \" \".join(querywords)\n",
    "    poslist = random.choices(querywords, k=ndps)\n",
    "    neglist = []\n",
    "    for _ in poslist:\n",
    "        while True: \n",
    "            neg = random.choice(wset)['text']\n",
    "            if neg not in querywords:\n",
    "                break\n",
    "        neglist.append(neg)\n",
    "    if datatype == \"train\":\n",
    "        return [{'query': query, 'positive': pos, 'negative': neg} for pos, neg in zip(poslist, neglist)]\n",
    "    else:\n",
    "        return {'question': query, 'pos_chunks': querywords, 'numposchunks': len(querywords)}\n",
    "\n",
    "nouns10k = Dataset.load_from_disk(\"../propercache/data/datastores/nouns10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toyquerydset(wset, qwords, ndps, tsize):\n",
    "    alldata = []\n",
    "    inddps = tsize // ndps\n",
    "    for _ in tqdm(range(inddps)):\n",
    "        alldata.extend(construct_toy_query(wset, qwords, ndps, \"train\"))\n",
    "    return Dataset.from_list(alldata)\n",
    "\n",
    "numwords = 100\n",
    "tset = 100000\n",
    "dp_per_query = 100\n",
    "traindata = DatasetDict({\n",
    "    'train': toyquerydset(nouns10k, numwords, dp_per_query, tset),\n",
    "    'test': toyquerydset(nouns10k, numwords, dp_per_query, 1000)\n",
    "})\n",
    "traindata.save_to_disk(f\"../propercache/data/colbert_training/nountraining{numwords}words{tset}ndps{dp_per_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48702939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate eval data following a probability distribution over count of query words\n",
    "def pdquerydset(wset, qwdistdict, tsize, ndps=1):\n",
    "    alldata = []\n",
    "    dcnt = 0\n",
    "    while dcnt < tsize:\n",
    "        qw = random.choices(list(qwdistdict.keys()), weights=list(qwdistdict.values()), k=1)[0]\n",
    "        alldata.extend(construct_toy_query(wset, qw, min(ndps, qw), \"train\"))\n",
    "        dcnt += 1\n",
    "        if dcnt % 10000 == 0:\n",
    "            print(f\"Generated {dcnt} queries\")\n",
    "    return Dataset.from_list(alldata)\n",
    "\n",
    "# uniform distribution from 5 to 100\n",
    "qwdistdictuni = {i: 1/96 for i in range(5, 101)}\n",
    "# power law distribution from 5 to 100\n",
    "qwdistdictpower = {i: i**(-0.5) for i in range(5, 101)}\n",
    "\n",
    "# plot power law distribution in qwdistdictpower\n",
    "x = list(qwdistdictpower.keys())\n",
    "y = list(qwdistdictpower.values())\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Number of Words in Query\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Power Law Distribution of Query Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 100k train set for each distribution\n",
    "train100kuniform = pdquerydset(nouns10k, qwdistdictuni, 100000)\n",
    "test100kuniform = pdquerydset(nouns10k, qwdistdictuni, 1000)\n",
    "\n",
    "train100kpower = pdquerydset(nouns10k, qwdistdictpower, 100000)\n",
    "test100kpower = pdquerydset(nouns10k, qwdistdictpower, 1000)\n",
    "\n",
    "DatasetDict({\n",
    "    'train': train100kuniform,\n",
    "    'test': test100kuniform\n",
    "}).save_to_disk(\"../propercache/data/colbert_training/nountraining100kuniform5_100\")\n",
    "\n",
    "DatasetDict({\n",
    "    'train': train100kpower,\n",
    "    'test': test100kpower\n",
    "}).save_to_disk(\"../propercache/data/colbert_training/nountraining100kpower5_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9051b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check distributions of 'query' length in train100kuniform, train100kpower\n",
    "plt.hist([len(q['query'].split()) for q in train100kuniform], alpha=0.5, label='Uniform', range=(5, 100))\n",
    "plt.hist([len(q['query'].split()) for q in train100kpower], alpha=0.5, label='Power', range=(5, 100))\n",
    "plt.ylabel(\"Count\") \n",
    "plt.xlabel(\"Number of Words in Query\")\n",
    "plt.title(\"Distribution of Query Length in Training Sets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e1b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae8805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evallists = [100] # [1, 5, 10, 25]\n",
    "for ev in evallists:\n",
    "    evdata = Dataset.from_list([construct_toy_query(test_nouns, ev, \"eval\") for _ in tqdm(range(1000))])\n",
    "    evdata.save_to_disk(f\"../propercache/data/evalsets/nountest{ev}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350760fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "evallists = [1, 5, 10, 25]\n",
    "testsets = {}\n",
    "for ev in evallists:\n",
    "    testsets[ev] = Dataset.load_from_disk(f\"../propercache/data/evalsets/nountest{ev}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "allnouns = Dataset.load_from_disk(\"../propercache/data/datastores/allnouns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a test set, convert it to a train set\n",
    "def test_to_trainset(tset):\n",
    "    alldata = []\n",
    "    for row in tset:\n",
    "        for pos in row['pos_chunks']:\n",
    "            alldata.append({'query': row['question'], 'positive': pos, 'negative': random.choice(allnouns)['text']})\n",
    "    return DatasetDict({'train': Dataset.from_list(alldata), 'test': Dataset.from_list(alldata)})\n",
    "\n",
    "tset = 25\n",
    "test_to_trainset(testsets[tset]).save_to_disk(f\"../propercache/data/colbert_training/nountestset{tset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398076b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_trainset(testsets[tset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32066cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval10 = Dataset.from_list([construct_toy_query(test_nouns, 10, \"eval\") for _ in tqdm(range(1000))])\n",
    "eval100 = Dataset.from_list([construct_toy_query(test_nouns, 100, \"eval\") for _ in tqdm(range(1000))])\n",
    "\n",
    "eval10.save_to_disk(\"../propercache/data/evalsets/nountest10/\")\n",
    "eval100.save_to_disk(\"../propercache/data/evalsets/nountest100/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4352c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO need to make eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata['train'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c2e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaling2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
